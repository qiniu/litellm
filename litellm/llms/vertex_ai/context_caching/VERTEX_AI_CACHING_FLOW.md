# Vertex AI Context Caching å®Œæ•´æµç¨‹æ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ç¼“å­˜æµç¨‹å›¾](#ç¼“å­˜æµç¨‹å›¾)
3. [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
4. [HTTP API æ¥å£](#http-api-æ¥å£)
5. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
6. [è¯¦ç»†æµç¨‹è¯´æ˜](#è¯¦ç»†æµç¨‹è¯´æ˜)
7. [ä»£ç è°ƒç”¨ç¤ºä¾‹](#ä»£ç è°ƒç”¨ç¤ºä¾‹)
8. [ä¼˜åŒ–ç‰ˆæœ¬](#ä¼˜åŒ–ç‰ˆæœ¬)

---

## æ¦‚è¿°

Vertex AI Context Caching å…è®¸æ‚¨ç¼“å­˜å¤§å‹ä¸Šä¸‹æ–‡ï¼ˆå¦‚ç³»ç»Ÿæç¤ºè¯ã€æ–‡æ¡£ç­‰ï¼‰ï¼Œé¿å…é‡å¤ä¼ è¾“ç›¸åŒå†…å®¹ï¼Œä»è€Œï¼š

- **é™ä½æˆæœ¬**ï¼šç¼“å­˜å†…å®¹æŒ‰æ›´ä½çš„ä»·æ ¼è®¡è´¹
- **å‡å°‘å»¶è¿Ÿ**ï¼šé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
- **æé«˜æ•ˆç‡**ï¼šç‰¹åˆ«é€‚åˆé•¿ä¸Šä¸‹æ–‡åœºæ™¯

### æ”¯æŒçš„æä¾›å•†

- **Vertex AI** (`vertex_ai`, `vertex_ai_beta`)
- **Google AI Studio** (`gemini`)

---

## ç¼“å­˜æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·è°ƒç”¨ completion()                                            â”‚
â”‚  messages ä¸­åŒ…å« cache_control æ ‡è®°                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æ¶ˆæ¯é¢„å¤„ç†                                                     â”‚
â”‚     - åˆ†ç¦»å¸¦ cache_control çš„æ¶ˆæ¯ (cached_messages)               â”‚
â”‚     - åˆ†ç¦»ä¸å¸¦ cache_control çš„æ¶ˆæ¯ (non_cached_messages)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ç”Ÿæˆç¼“å­˜é”® (Cache Key)                                        â”‚
â”‚     - åŸºäº cached_messages + tools ç”Ÿæˆå“ˆå¸Œ                       â”‚
â”‚     - cache_key = hash(messages + tools)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ (ä¼˜åŒ–ç‰ˆæœ¬)                                        â”‚
â”‚     - æŸ¥è¯¢æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨                                           â”‚
â”‚     - ä½¿ç”¨ scoped_key = cache_key:project:location               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚
        â–¼ æ‰¾åˆ°              â–¼ æœªæ‰¾åˆ°
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¿”å›         â”‚     â”‚  4. æŸ¥è¯¢ Google API                  â”‚
â”‚ cache_id    â”‚     â”‚     GET cachedContents (List All)   â”‚
â”‚             â”‚     â”‚     æ£€æŸ¥ displayName == cache_key    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
                    â–¼ æ‰¾åˆ°              â–¼ æœªæ‰¾åˆ°
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ è¿”å›         â”‚     â”‚  5. åˆ›å»ºæ–°ç¼“å­˜             â”‚
            â”‚ cache_id    â”‚     â”‚     POST cachedContents   â”‚
            â”‚             â”‚     â”‚     å­˜å…¥æœ¬åœ°ç¼“å­˜           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚  è¿”å›æ–°çš„ cache_id        â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. å‘èµ·å®é™…çš„ Completion è¯·æ±‚                                     â”‚
â”‚     - ä½¿ç”¨ non_cached_messages                                   â”‚
â”‚     - æºå¸¦ cached_content = cache_id                             â”‚
â”‚     - Google è‡ªåŠ¨ä»ç¼“å­˜åŠ è½½ä¸Šä¸‹æ–‡                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒç»„ä»¶

### 1. ContextCachingEndpoints

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

ä¸»è¦è´Ÿè´£ç¼“å­˜çš„æ£€æŸ¥ã€åˆ›å»ºå’Œç®¡ç†ã€‚

#### æ ¸å¿ƒæ–¹æ³•

```python
class ContextCachingEndpoints(VertexBase):

    def check_and_create_cache(
        self,
        messages: List[AllMessageValues],      # OpenAI æ ¼å¼æ¶ˆæ¯
        optional_params: dict,                 # åŒ…å« tools ç­‰å‚æ•°
        api_key: str,                          # API å¯†é’¥
        api_base: Optional[str],               # API åŸºç¡€åœ°å€
        model: str,                            # æ¨¡å‹åç§°
        client: Optional[HTTPHandler],         # HTTP å®¢æˆ·ç«¯
        timeout: Optional[Union[float, httpx.Timeout]],
        logging_obj: Logging,                  # æ—¥å¿—å¯¹è±¡
        custom_llm_provider: Literal["vertex_ai", "vertex_ai_beta", "gemini"],
        vertex_project: Optional[str],         # Vertex AI é¡¹ç›® ID
        vertex_location: Optional[str],        # Vertex AI åŒºåŸŸ
        vertex_auth_header: Optional[str],     # è®¤è¯å¤´
        extra_headers: Optional[dict] = None,
        cached_content: Optional[str] = None,  # å·²æœ‰çš„ cache_id
    ) -> Tuple[List[AllMessageValues], dict, Optional[str]]:
        """
        æ£€æŸ¥å¹¶åˆ›å»ºç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰

        è¿”å›:
            - non_cached_messages: ä¸éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯åˆ—è¡¨
            - optional_params: æ›´æ–°åçš„å‚æ•°ï¼ˆç§»é™¤äº† toolsï¼‰
            - cache_id: ç¼“å­˜ IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
```

#### è¾…åŠ©æ–¹æ³•

```python
def check_cache(
    self,
    cache_key: str,                            # ç¼“å­˜é”®ï¼ˆdisplayNameï¼‰
    client: HTTPHandler,
    headers: dict,
    ...
) -> Optional[str]:
    """
    æ£€æŸ¥ Google API ä¸­æ˜¯å¦å·²å­˜åœ¨ç¼“å­˜

    è¿”å›:
        - cache_id: ç¼“å­˜ IDï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
        - None: å¦‚æœæœªæ‰¾åˆ°
    """
```

### 2. LocalCacheManager (ä¼˜åŒ–ç‰ˆæœ¬)

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/local_cache_manager.py`

æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨ï¼Œé¿å…é‡å¤çš„ç½‘ç»œè¯·æ±‚ã€‚

```python
class LocalCacheManager:
    """çº¿ç¨‹å®‰å…¨çš„æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨"""

    def get_cache(
        self,
        cache_key: str,
        vertex_project: Optional[str] = None,
        vertex_location: Optional[str] = None,
        custom_llm_provider: Optional[str] = None
    ) -> Optional[str]:
        """è·å–ç¼“å­˜ IDï¼ˆå¦‚æœå­˜åœ¨ä¸”æœªè¿‡æœŸï¼‰"""

    def set_cache(
        self,
        cache_key: str,
        cache_id: str,
        ttl_seconds: float,
        vertex_project: Optional[str] = None,
        vertex_location: Optional[str] = None,
        custom_llm_provider: Optional[str] = None
    ) -> None:
        """å­˜å‚¨ç¼“å­˜æ˜ å°„"""
```

### 3. Transformation æ¨¡å—

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/transformation.py`

è´Ÿè´£æ¶ˆæ¯æ ¼å¼è½¬æ¢å’Œç¼“å­˜æ¶ˆæ¯åˆ†ç¦»ã€‚

```python
def separate_cached_messages(
    messages: List[AllMessageValues]
) -> Tuple[List[AllMessageValues], List[AllMessageValues]]:
    """
    åˆ†ç¦»å¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯

    è¿”å›:
        - cached_messages: å¸¦ cache_control çš„æ¶ˆæ¯
        - non_cached_messages: æ™®é€šæ¶ˆæ¯
    """

def transform_openai_messages_to_gemini_context_caching(
    model: str,
    messages: List[AllMessageValues],
    custom_llm_provider: Literal["vertex_ai", "vertex_ai_beta", "gemini"],
    cache_key: str,
    vertex_project: Optional[str],
    vertex_location: Optional[str],
) -> CachedContentRequestBody:
    """
    å°† OpenAI æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º Gemini ç¼“å­˜è¯·æ±‚æ ¼å¼
    """
```

---

## HTTP API æ¥å£

### 1. åˆ—å‡ºæ‰€æœ‰ç¼“å­˜ (List Cached Contents)

#### Vertex AI

**è¯·æ±‚**

```http
GET https://aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents
Authorization: Bearer {ACCESS_TOKEN}
```

å¯¹äºé global åŒºåŸŸï¼š
```http
GET https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents
Authorization: Bearer {ACCESS_TOKEN}
```

**å“åº”ç¤ºä¾‹**

```json
{
  "cachedContents": [
    {
      "name": "projects/123/locations/global/cachedContents/abc-123",
      "model": "projects/123/locations/global/publishers/google/models/gemini-2.0-flash-001",
      "displayName": "cache-key-hash-xyz",
      "createTime": "2024-12-11T00:00:00Z",
      "updateTime": "2024-12-11T00:00:00Z",
      "expireTime": "2024-12-11T01:00:00Z",
      "usageMetadata": {
        "totalTokenCount": 2048
      }
    }
  ]
}
```

#### Google AI Studio

**è¯·æ±‚**

```http
GET https://generativelanguage.googleapis.com/v1beta/cachedContents?key={API_KEY}
```

**å“åº”æ ¼å¼ç›¸åŒ**

### 2. åˆ›å»ºç¼“å­˜ (Create Cached Content)

#### Vertex AI

**è¯·æ±‚**

```http
POST https://aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents
Authorization: Bearer {ACCESS_TOKEN}
Content-Type: application/json

{
  "model": "projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-2.0-flash-001",
  "displayName": "cache-key-hash-xyz",
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "è¿™æ˜¯è¦ç¼“å­˜çš„é•¿æ–‡æœ¬å†…å®¹..."
        }
      ]
    }
  ],
  "ttl": "3600s",
  "systemInstruction": {
    "parts": [
      {
        "text": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"
      }
    ]
  },
  "tools": [
    {
      "functionDeclarations": [...]
    }
  ]
}
```

**å“åº”ç¤ºä¾‹**

```json
{
  "name": "projects/123/locations/global/cachedContents/abc-123",
  "model": "projects/123/locations/global/publishers/google/models/gemini-2.0-flash-001",
  "displayName": "cache-key-hash-xyz",
  "createTime": "2024-12-11T00:00:00Z",
  "updateTime": "2024-12-11T00:00:00Z",
  "expireTime": "2024-12-11T01:00:00Z",
  "usageMetadata": {
    "totalTokenCount": 2048
  }
}
```

#### Google AI Studio

**è¯·æ±‚**

```http
POST https://generativelanguage.googleapis.com/v1beta/cachedContents?key={API_KEY}
Content-Type: application/json

{
  "model": "models/gemini-2.0-flash-001",
  "displayName": "cache-key-hash-xyz",
  "contents": [...],
  "ttl": "3600s"
}
```

### 3. ä½¿ç”¨ç¼“å­˜å‘èµ· Completion è¯·æ±‚

#### Vertex AI

**è¯·æ±‚**

```http
POST https://aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-2.0-flash-001:generateContent
Authorization: Bearer {ACCESS_TOKEN}
Content-Type: application/json

{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "åŸºäºä¹‹å‰çš„ä¸Šä¸‹æ–‡ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜..."
        }
      ]
    }
  ],
  "cachedContent": "projects/123/locations/global/cachedContents/abc-123"
}
```

**è¯´æ˜**ï¼š
- `cachedContent` å­—æ®µæŒ‡å®šè¦ä½¿ç”¨çš„ç¼“å­˜ ID
- Google ä¼šè‡ªåŠ¨ä»ç¼“å­˜åŠ è½½ä¹‹å‰çš„ä¸Šä¸‹æ–‡
- `contents` ä¸­åªéœ€è¦åŒ…å«æ–°çš„æ¶ˆæ¯

---

## ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ 1: é€šè¿‡ LiteLLM SDK (æ¨è)

#### åŸºç¡€ç”¨æ³•

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ç³»ç»Ÿæç¤ºè¯ï¼ŒåŒ…å«å¤§é‡ä¸Šä¸‹æ–‡ä¿¡æ¯...",
                "cache_control": {
                    "type": "ephemeral",
                    "ttl": "3600s"  # ç¼“å­˜ 1 å°æ—¶
                }
            }
        ]
    },
    {
        "role": "user",
        "content": "åŸºäºä¸Šé¢çš„ä¸Šä¸‹æ–‡ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜..."
    }
]

# ç¬¬ä¸€æ¬¡è°ƒç”¨ - åˆ›å»ºç¼“å­˜
response = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    vertex_credentials="/path/to/credentials.json"
)

# ç¬¬äºŒæ¬¡è°ƒç”¨ - ä½¿ç”¨ç¼“å­˜ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
# åªè¦ cached_messages å†…å®¹ç›¸åŒï¼Œå°±ä¼šå¤ç”¨ç¼“å­˜
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,  # ç›¸åŒçš„ç¼“å­˜å†…å®¹
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    vertex_credentials="/path/to/credentials.json"
)
```

#### å¼‚æ­¥ç”¨æ³•

```python
from litellm import acompletion
import asyncio

async def main():
    messages = [...]  # åŒä¸Š

    response = await acompletion(
        model="vertex_ai/gemini-2.0-flash-001",
        messages=messages,
        vertex_project="gemini-qn-bz",
        vertex_location="global",
        vertex_credentials="/path/to/credentials.json"
    )

    print(response.choices[0].message.content)

asyncio.run(main())
```

#### å¤šé¡¹ç›®é…ç½®

```python
# é¡¹ç›® 1
response1 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)

# é¡¹ç›® 2 - ç›¸åŒå†…å®¹ï¼Œç‹¬ç«‹ç¼“å­˜
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-prod",
    vertex_location="global"
)

# âœ… ä¸¤ä¸ªé¡¹ç›®å„è‡ªç»´æŠ¤ç‹¬ç«‹çš„ç¼“å­˜
```

### æ–¹å¼ 2: é€šè¿‡ LiteLLM Proxy

#### é…ç½®æ–‡ä»¶

```yaml
model_list:
  - model_name: gemini-2.0-flash
    litellm_params:
      model: vertex_ai/gemini-2.0-flash-001
      vertex_project: "gemini-qn-bz"
      vertex_location: "global"
      vertex_credentials: /app/gemini-bz1.json
```

#### å®¢æˆ·ç«¯è°ƒç”¨

```python
import openai

client = openai.OpenAI(
    api_key="sk-1234",  # LiteLLM proxy key
    base_url="http://localhost:4000"
)

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "é•¿æ–‡æœ¬å†…å®¹...",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }
        ]
    },
    {"role": "user", "content": "é—®é¢˜"}
]

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=messages
)
```

### æ–¹å¼ 3: Google AI Studio (Gemini)

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "é•¿æ–‡æœ¬å†…å®¹...",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }
        ]
    },
    {"role": "user", "content": "é—®é¢˜"}
]

response = completion(
    model="gemini/gemini-2.0-flash-001",
    messages=messages,
    api_key="YOUR_GEMINI_API_KEY"
)
```

---

## è¯¦ç»†æµç¨‹è¯´æ˜

### æ­¥éª¤ 1: æ¶ˆæ¯åˆ†ç¦»

å½“ç”¨æˆ·è°ƒç”¨ `completion()` æ—¶ï¼Œå¦‚æœ messages ä¸­åŒ…å« `cache_control` æ ‡è®°ï¼š

```python
# è¾“å…¥æ¶ˆæ¯
messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "ç³»ç»Ÿæç¤ºè¯...",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }
        ]
    },
    {"role": "user", "content": "æ–‡æ¡£å†…å®¹..."},
    {"role": "user", "content": "é—®é¢˜"}
]

# åˆ†ç¦»å
cached_messages = [messages[0], messages[1]]      # å¸¦ cache_control çš„
non_cached_messages = [messages[2]]               # ä¸å¸¦ cache_control çš„
```

**è°ƒç”¨ä½ç½®**: `litellm/llms/vertex_ai/gemini/transformation.py:575`

```python
from litellm.llms.vertex_ai.context_caching import ContextCachingEndpoints

context_caching_endpoints = ContextCachingEndpoints()

messages, optional_params, cached_content = context_caching_endpoints.check_and_create_cache(
    messages=messages,
    optional_params=optional_params,
    api_key=gemini_api_key or "dummy",
    api_base=api_base,
    model=model,
    client=client,
    timeout=timeout,
    extra_headers=extra_headers,
    cached_content=optional_params.pop("cached_content", None),
    logging_obj=logging_obj,
    custom_llm_provider=custom_llm_provider,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
    vertex_auth_header=vertex_auth_header
)
```

### æ­¥éª¤ 2: ç”Ÿæˆç¼“å­˜é”®

ä½¿ç”¨ cached_messages + tools ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®ï¼š

```python
from litellm.caching.caching import Cache, LiteLLMCacheType

local_cache_obj = Cache(type=LiteLLMCacheType.LOCAL)

# ç”Ÿæˆç¼“å­˜é”®
cache_key = local_cache_obj.get_cache_key(
    messages=cached_messages,
    tools=tools
)
# ç¤ºä¾‹: "cache-key-a1b2c3d4e5f6"
```

**ä»£ç ä½ç½®**: `vertex_ai_context_caching.py:306-308`

### æ­¥éª¤ 3: æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

cache_manager = get_cache_manager()

# æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆå¸¦é¡¹ç›®ä½œç”¨åŸŸï¼‰
local_cache_id = cache_manager.get_cache(
    cache_key=cache_key,
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)

if local_cache_id is not None:
    # æ‰¾åˆ°æœ¬åœ°ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    return non_cached_messages, optional_params, local_cache_id
```

**ä¼˜åŠ¿**ï¼š
- æ— ç½‘ç»œè¯·æ±‚
- å“åº”æ—¶é—´ < 1ms
- èŠ‚çœ 60-80% ç½‘ç»œè°ƒç”¨

### æ­¥éª¤ 4: æ£€æŸ¥ Google ç¼“å­˜

å¦‚æœæœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼ŒæŸ¥è¯¢ Google APIï¼š

```python
def check_cache(self, cache_key, ...):
    # GET è¯·æ±‚åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
    url = f"https://aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/cachedContents"

    resp = client.get(url=url, headers=headers)
    raw_response = resp.json()

    # æŸ¥æ‰¾ displayName åŒ¹é…çš„ç¼“å­˜
    for cached_item in raw_response["cachedContents"]:
        if cached_item.get("displayName") == cache_key:
            cache_id = cached_item.get("name")

            # å­˜å…¥æœ¬åœ°ç¼“å­˜ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            cache_manager.set_cache(
                cache_key=cache_key,
                cache_id=cache_id,
                ttl_seconds=3600.0,
                vertex_project=vertex_project,
                vertex_location=vertex_location,
                custom_llm_provider=custom_llm_provider
            )

            return cache_id

    return None
```

**ä»£ç ä½ç½®**: `vertex_ai_context_caching.py:94-164`

### æ­¥éª¤ 5: åˆ›å»ºæ–°ç¼“å­˜

å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç¼“å­˜ï¼š

```python
# è½¬æ¢æ¶ˆæ¯æ ¼å¼
cached_content_request_body = transform_openai_messages_to_gemini_context_caching(
    model=model,
    messages=cached_messages,
    cache_key=cache_key,
    custom_llm_provider=custom_llm_provider,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
)

cached_content_request_body["tools"] = tools

# POST åˆ›å»ºç¼“å­˜
url = f"https://aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/cachedContents"

response = client.post(url=url, headers=headers, json=cached_content_request_body)
raw_response = response.json()

cache_id = raw_response["name"]

# å­˜å…¥æœ¬åœ°ç¼“å­˜ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
ttl_seconds = parse_ttl_to_seconds(cached_content_request_body.get("ttl", "3600s"))
cache_manager.set_cache(
    cache_key=cache_key,
    cache_id=cache_id,
    ttl_seconds=ttl_seconds,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
    custom_llm_provider=custom_llm_provider
)

return non_cached_messages, optional_params, cache_id
```

**ä»£ç ä½ç½®**: `vertex_ai_context_caching.py:324-368`

### æ­¥éª¤ 6: ä½¿ç”¨ç¼“å­˜å‘èµ·è¯·æ±‚

ç¼“å­˜å¤„ç†å®Œæˆåï¼Œè¿”å›åˆ°ä¸»æµç¨‹ï¼š

```python
# check_and_create_cache è¿”å›
messages = non_cached_messages  # åªåŒ…å«ä¸éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯
cached_content = cache_id        # ç¼“å­˜ ID

# æ„é€ è¯·æ±‚ä½“
data = {
    "contents": transform_messages(non_cached_messages),
    "cachedContent": cached_content,  # å…³é”®ï¼šæŒ‡å®šç¼“å­˜ ID
    "generationConfig": {...}
}

# å‘èµ·å®é™…çš„ç”Ÿæˆè¯·æ±‚
url = f"https://aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/{model}:generateContent"

response = client.post(url=url, json=data)
```

Google ä¼šè‡ªåŠ¨ï¼š
1. ä» `cachedContent` ID åŠ è½½ç¼“å­˜çš„ä¸Šä¸‹æ–‡
2. å°†ç¼“å­˜ä¸Šä¸‹æ–‡ + æ–°æ¶ˆæ¯ç»„åˆå¤„ç†
3. åªå¯¹æ–°æ¶ˆæ¯éƒ¨åˆ†è®¡è´¹ï¼ˆç¼“å­˜éƒ¨åˆ†æŒ‰æ›´ä½ä»·æ ¼è®¡è´¹ï¼‰

---

## ä»£ç è°ƒç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ç¼“å­˜ä½¿ç”¨

```python
from litellm import completion

# å®šä¹‰å¸¦ç¼“å­˜çš„æ¶ˆæ¯
messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹ï¼Œç²¾é€š Pythonã€JavaScript å’Œäº‘è®¡ç®—ã€‚",
                "cache_control": {
                    "type": "ephemeral",
                    "ttl": "7200s"  # ç¼“å­˜ 2 å°æ—¶
                }
            }
        ]
    },
    {
        "role": "user",
        "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ Vertex AIï¼Ÿ"
    }
]

# ç¬¬ä¸€æ¬¡è°ƒç”¨ - åˆ›å»ºç¼“å­˜
print("ç¬¬ä¸€æ¬¡è°ƒç”¨...")
response1 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    vertex_credentials="/path/to/credentials.json"
)
print(f"å“åº”: {response1.choices[0].message.content}")
print(f"è€—æ—¶: ~1.5ç§’ï¼ˆåŒ…å«ç¼“å­˜åˆ›å»ºï¼‰")

# ç¬¬äºŒæ¬¡è°ƒç”¨ - ä½¿ç”¨ç¼“å­˜
print("\nç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç›¸åŒä¸Šä¸‹æ–‡ï¼‰...")
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    vertex_credentials="/path/to/credentials.json"
)
print(f"å“åº”: {response2.choices[0].message.content}")
print(f"è€—æ—¶: ~0.3ç§’ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
```

### ç¤ºä¾‹ 2: ç¼“å­˜é•¿æ–‡æ¡£

```python
from litellm import completion

# è¯»å–é•¿æ–‡æ¡£
with open("long_document.txt", "r") as f:
    document = f.read()

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": f"æ–‡æ¡£å†…å®¹:\n\n{document}",
                "cache_control": {
                    "type": "ephemeral",
                    "ttl": "3600s"
                }
            }
        ]
    },
    {
        "role": "user",
        "content": "æ€»ç»“è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹"
    }
]

response = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)

print(response.choices[0].message.content)

# åç»­å¯ä»¥ç»§ç»­æé—®ï¼Œå¤ç”¨ç¼“å­˜çš„æ–‡æ¡£
messages.append({"role": "assistant", "content": response.choices[0].message.content})
messages.append({"role": "user", "content": "è¯¦ç»†è§£é‡Šç¬¬ä¸€éƒ¨åˆ†"})

response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)
```

### ç¤ºä¾‹ 3: ç¼“å­˜ Tools å®šä¹‰

```python
from litellm import completion

# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "åŸå¸‚åç§°"}
                },
                "required": ["city"]
            }
        }
    },
    # ... æ›´å¤šå·¥å…·å®šä¹‰
]

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "ä½ æ˜¯ä¸€ä¸ªå¤©æ°”åŠ©æ‰‹ï¼Œå¯ä»¥æŸ¥è¯¢å¤©æ°”ä¿¡æ¯ã€‚",
                "cache_control": {
                    "type": "ephemeral",
                    "ttl": "3600s"
                }
            }
        ]
    },
    {
        "role": "user",
        "content": "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    }
]

# Tools ä¹Ÿä¼šè¢«åŒ…å«åœ¨ç¼“å­˜ä¸­
response = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    tools=tools,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)
```

### ç¤ºä¾‹ 4: ç›‘æ§ç¼“å­˜ä½¿ç”¨

```python
from litellm import completion
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

# è·å–ç¼“å­˜ç®¡ç†å™¨
cache_manager = get_cache_manager()

# æ¸…ç©ºç¼“å­˜ï¼ˆå¯é€‰ï¼‰
cache_manager.clear_all()

messages = [...]  # å®šä¹‰æ¶ˆæ¯

# ç¬¬ä¸€æ¬¡è°ƒç”¨
response1 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = cache_manager.get_stats()
print(f"æ€»ç¼“å­˜æ¡ç›®: {stats['total_entries']}")
print(f"æœ‰æ•ˆæ¡ç›®: {stats['valid_entries']}")
print(f"ç¼“å­˜é”®åˆ—è¡¨:")
for key in stats['cache_keys']:
    print(f"  - {key}")

# ç¬¬äºŒæ¬¡è°ƒç”¨
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)

# éªŒè¯ä½¿ç”¨äº†ç¼“å­˜
stats2 = cache_manager.get_stats()
print(f"\nè°ƒç”¨åç»Ÿè®¡:")
print(f"æ€»æ¡ç›®: {stats2['total_entries']} (åº”è¯¥ç›¸åŒ)")
```

### ç¤ºä¾‹ 5: å¤šé¡¹ç›®éš”ç¦»

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "ç›¸åŒçš„ç³»ç»Ÿæç¤ºè¯",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }
        ]
    },
    {"role": "user", "content": "æµ‹è¯•é—®é¢˜"}
]

# é¡¹ç›® 1
response1 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-qn-bz",
    vertex_location="global"
)
print(f"é¡¹ç›® 1 å“åº”: {response1.choices[0].message.content}")

# é¡¹ç›® 2 - ç›¸åŒå†…å®¹ï¼Œç‹¬ç«‹ç¼“å­˜
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="gemini-prod",
    vertex_location="global"
)
print(f"é¡¹ç›® 2 å“åº”: {response2.choices[0].message.content}")

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager
stats = get_cache_manager().get_stats()

print(f"\nç¼“å­˜æ¡ç›®æ•°: {stats['total_entries']}")  # åº”è¯¥æ˜¯ 2
for key in stats['cache_keys']:
    print(f"  {key}")
    # è¾“å‡ºç¤ºä¾‹:
    # cache-key-abc:gemini-qn-bz:global:7c0ff9df
    # cache-key-abc:gemini-prod:global:225155362
```

---

## ä¼˜åŒ–ç‰ˆæœ¬

### æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨

ä¼˜åŒ–ç‰ˆæœ¬å¢åŠ äº†æœ¬åœ°ç¼“å­˜å±‚ï¼Œé¿å…é‡å¤çš„ç½‘ç»œè¯·æ±‚ï¼š

**æ–‡ä»¶**: `vertex_ai_context_caching_optimized.py`

#### ä¼˜åŒ–ç‚¹ 1: æœ¬åœ°ç¼“å­˜æ£€æŸ¥

```python
class ContextCachingEndpointsOptimized(VertexBase):
    def __init__(self):
        self.local_cache_manager = get_cache_manager()

    def check_and_create_cache(self, ...):
        # ... åˆ†ç¦»æ¶ˆæ¯ ...

        # âœ… ä¼˜åŒ– 1: å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆæ— ç½‘ç»œè¯·æ±‚ï¼‰
        local_cache_id = self.local_cache_manager.get_cache(
            cache_key=generated_cache_key,
            vertex_project=vertex_project,
            vertex_location=vertex_location,
            custom_llm_provider=custom_llm_provider
        )

        if local_cache_id is not None:
            # æœ¬åœ°å‘½ä¸­ï¼Œç›´æ¥è¿”å›ï¼ˆ0.3ç§’ vs 1.5ç§’ï¼‰
            return non_cached_messages, optional_params, local_cache_id

        # æœ¬åœ°æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢ Google API
        google_cache_name = self.check_cache(...)

        if google_cache_name:
            return non_cached_messages, optional_params, google_cache_name

        # åˆ›å»ºæ–°ç¼“å­˜
        # ...

        # âœ… ä¼˜åŒ– 2: å­˜å…¥æœ¬åœ°ç¼“å­˜
        self.local_cache_manager.set_cache(
            cache_key=generated_cache_key,
            cache_id=cache_id,
            ttl_seconds=ttl_seconds,
            vertex_project=vertex_project,
            vertex_location=vertex_location,
            custom_llm_provider=custom_llm_provider
        )
```

#### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | åŸå§‹å®ç° | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡ |
|------|---------|---------|------|
| é¦–æ¬¡è¯·æ±‚ | 1.5ç§’ | 1.5ç§’ | 0% |
| ç¼“å­˜å‘½ä¸­ | 0.8ç§’ | 0.3ç§’ | **62% â†“** |
| ç½‘ç»œè°ƒç”¨ï¼ˆ3æ¬¡è¯·æ±‚ï¼‰ | 6æ¬¡ | 2æ¬¡ | **66% â†“** |

#### å¤šé¡¹ç›®åœºæ™¯æ”¶ç›Š

å‡è®¾ï¼š
- 3 ä¸ªé¡¹ç›®
- æ¯åˆ†é’Ÿ 100 è¯·æ±‚
- 80% ç¼“å­˜å‘½ä¸­ç‡

**åŸå§‹å®ç°**:
- æ¯ä¸ªé¡¹ç›®: 100 æ¬¡ç½‘ç»œè°ƒç”¨
- æ€»è®¡: **300 æ¬¡/åˆ†é’Ÿ**

**ä¼˜åŒ–å**:
- æ¯ä¸ªé¡¹ç›®: 20 æ¬¡ç½‘ç»œè°ƒç”¨ï¼ˆåªåœ¨æœªå‘½ä¸­æ—¶ï¼‰
- æ€»è®¡: **60 æ¬¡/åˆ†é’Ÿ**

**èŠ‚çœ**: 240 æ¬¡ç½‘ç»œè°ƒç”¨/åˆ†é’Ÿ (80% â†“)

### ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬

åªéœ€è¦æ›¿æ¢å¯¼å…¥ï¼š

```python
# åŸå§‹ç‰ˆæœ¬
from litellm.llms.vertex_ai.context_caching import ContextCachingEndpoints

# ä¼˜åŒ–ç‰ˆæœ¬
from litellm.llms.vertex_ai.context_caching.vertex_ai_context_caching_optimized import ContextCachingEndpointsOptimized

# ä½¿ç”¨æ–¹å¼å®Œå…¨ç›¸åŒ
context_caching = ContextCachingEndpointsOptimized()
```

æˆ–è€…æŒ‰ç…§ `OPTIMIZATION_SUMMARY.md` ä¸­çš„æœ€å°åŒ–ä¿®æ”¹æ–¹æ¡ˆé›†æˆåˆ°ç°æœ‰ä»£ç ã€‚

---

## ç›¸å…³æ–‡æ¡£

1. **[MULTI_PROJECT_CACHE_GUIDE.md](./MULTI_PROJECT_CACHE_GUIDE.md)** - å¤šé¡¹ç›®ç¼“å­˜éš”ç¦»è¯¦ç»†æŒ‡å—
2. **[CACHE_OPTIMIZATION_GUIDE.md](./CACHE_OPTIMIZATION_GUIDE.md)** - å®Œæ•´ä¼˜åŒ–å®ç°æŒ‡å—
3. **[OPTIMIZATION_SUMMARY.md](./OPTIMIZATION_SUMMARY.md)** - å¿«é€Ÿé›†æˆæŒ‡å—
4. **[FINAL_SUMMARY.md](./FINAL_SUMMARY.md)** - å®Œæ•´æ–¹æ¡ˆæ€»ç»“

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ¤æ–­ç¼“å­˜æ˜¯å¦è¢«ä½¿ç”¨ï¼Ÿ

æŸ¥çœ‹æ—¥å¿—æˆ–ä½¿ç”¨æœ¬åœ°ç¼“å­˜ç»Ÿè®¡ï¼š

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

stats = get_cache_manager().get_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡: {stats['valid_entries']} ä¸ªæœ‰æ•ˆç¼“å­˜")
```

### Q2: ç¼“å­˜å¤šä¹…è¿‡æœŸï¼Ÿ

ç”± `ttl` å‚æ•°æ§åˆ¶ï¼š

```python
"cache_control": {
    "type": "ephemeral",
    "ttl": "3600s"  # 1 å°æ—¶åè¿‡æœŸ
}
```

æœ€çŸ­ 60 ç§’ï¼Œæœ€é•¿ 24 å°æ—¶ã€‚

### Q3: å¦‚ä½•æ¸…é™¤ç¼“å­˜ï¼Ÿ

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

# æ¸…é™¤æœ¬åœ°ç¼“å­˜
get_cache_manager().clear_all()

# Google ç¼“å­˜ä¼šè‡ªåŠ¨è¿‡æœŸï¼Œæˆ–é€šè¿‡ DELETE API åˆ é™¤
```

### Q4: å¤šä¸ªé¡¹ç›®çš„ç¼“å­˜ä¼šå†²çªå—ï¼Ÿ

ä¸ä¼šã€‚ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨ `project:location` ä½œä¸ºä½œç”¨åŸŸï¼Œå®Œå…¨éš”ç¦»ã€‚

```python
# ç›¸åŒå†…å®¹ï¼Œä¸åŒé¡¹ç›® = ç‹¬ç«‹ç¼“å­˜
cache_key_1 = "content-hash:gemini-qn-bz:global:xxx"
cache_key_2 = "content-hash:gemini-prod:global:yyy"
```

### Q5: æœ¬åœ°ç¼“å­˜æ˜¯è¿›ç¨‹çº§åˆ«è¿˜æ˜¯å…¨å±€çš„ï¼Ÿ

è¿›ç¨‹çº§åˆ«ã€‚æ¯ä¸ªè¿›ç¨‹ç»´æŠ¤ç‹¬ç«‹çš„æœ¬åœ°ç¼“å­˜ã€‚

æœªæ¥å¯ä»¥é€šè¿‡ Redis å®ç°è·¨è¿›ç¨‹å…±äº«ã€‚

---

## æ€»ç»“

Vertex AI Context Caching é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¼˜åŒ–æ€§èƒ½ï¼š

1. **å‡å°‘é‡å¤ä¼ è¾“**ï¼šç¼“å­˜å¤§å‹ä¸Šä¸‹æ–‡
2. **é™ä½æˆæœ¬**ï¼šç¼“å­˜å†…å®¹æŒ‰æ›´ä½ä»·æ ¼è®¡è´¹
3. **æœ¬åœ°ä¼˜åŒ–**ï¼šé¿å…é‡å¤çš„ç½‘ç»œæŸ¥è¯¢
4. **å¤šé¡¹ç›®éš”ç¦»**ï¼šç¡®ä¿ä¸åŒé¡¹ç›®ç¼“å­˜ç‹¬ç«‹

å»ºè®®ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¯è·å¾— 60-80% çš„æ€§èƒ½æå‡ï¼

**ç«‹å³å¼€å§‹**: å‚è€ƒ [OPTIMIZATION_SUMMARY.md](./OPTIMIZATION_SUMMARY.md) å¿«é€Ÿé›†æˆã€‚
