# Vertex AI Context Caching ä»£ç é€»è¾‘æ¢³ç†

## ğŸ“‹ ç›®å½•

1. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
2. [æ ¸å¿ƒæµç¨‹](#æ ¸å¿ƒæµç¨‹)
3. [å…³é”®ç»„ä»¶è¯¦è§£](#å…³é”®ç»„ä»¶è¯¦è§£)
4. [æ•°æ®æµè½¬](#æ•°æ®æµè½¬)
5. [ä¼˜åŒ–æœºåˆ¶](#ä¼˜åŒ–æœºåˆ¶)
6. [åŒæ­¥ä¸å¼‚æ­¥ç‰ˆæœ¬](#åŒæ­¥ä¸å¼‚æ­¥ç‰ˆæœ¬)
7. [é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ](#é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ)
8. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)

---

## æ•´ä½“æ¶æ„

Vertex AI Context Caching çš„ä»£ç ä¸»è¦åˆ†å¸ƒåœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­ï¼š

```
litellm/llms/vertex_ai/
â”œâ”€â”€ context_caching/
â”‚   â”œâ”€â”€ vertex_ai_context_caching.py          # æ ¸å¿ƒç¼“å­˜é€»è¾‘ï¼ˆå·²åŒ…å«æ‰€æœ‰ä¼˜åŒ–ï¼‰
â”‚   â”œâ”€â”€ local_cache_manager.py                # æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨
â”‚   â””â”€â”€ transformation.py                    # æ¶ˆæ¯è½¬æ¢å’Œåˆ†ç¦»é€»è¾‘
â””â”€â”€ gemini/
    â”œâ”€â”€ vertex_and_google_ai_studio_gemini.py # Gemini ä¸»å®ç°
    â””â”€â”€ transformation.py                     # è¯·æ±‚ä½“è½¬æ¢ï¼ˆé›†æˆç‚¹ï¼‰
```

### è°ƒç”¨é“¾è·¯

#### åŒæ­¥ç‰ˆæœ¬è°ƒç”¨é“¾è·¯

```
ç”¨æˆ·è°ƒç”¨ litellm.completion()
    â†“
litellm/main.py:completion()
    â†“
litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:completion()
    â†“
litellm/llms/vertex_ai/gemini/transformation.py:sync_transform_request_body()
    â†“
litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py:check_and_create_cache()
    â†“
è¿”å› (non_cached_messages, optional_params, cache_id)
    â†“
æ„é€  Gemini API è¯·æ±‚ï¼ˆåŒ…å« cachedContent å­—æ®µï¼‰
```

#### å¼‚æ­¥ç‰ˆæœ¬è°ƒç”¨é“¾è·¯

```
ç”¨æˆ·è°ƒç”¨ litellm.acompletion()
    â†“
litellm/main.py:acompletion()
    â†“
litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:async_completion()
    â†“
litellm/llms/vertex_ai/gemini/transformation.py:async_transform_request_body()
    â†“
litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py:async_check_and_create_cache()
    â†“
è¿”å› (non_cached_messages, optional_params, cache_id)
    â†“
æ„é€  Gemini API è¯·æ±‚ï¼ˆåŒ…å« cachedContent å­—æ®µï¼‰
```

---

## æ ¸å¿ƒæµç¨‹

### 1. å…¥å£ï¼šæ¶ˆæ¯è½¬æ¢é˜¶æ®µ

**æ–‡ä»¶**: `litellm/llms/vertex_ai/gemini/transformation.py`

åœ¨æ„é€  Gemini API è¯·æ±‚ä½“ä¹‹å‰ï¼Œä¼šå…ˆè°ƒç”¨ context caching é€»è¾‘ï¼š

**é‡è¦**ï¼šå¦‚æœ `optional_params` ä¸­å·²æœ‰ `cached_content`ï¼Œä¼šç›´æ¥ä½¿ç”¨ï¼Œè·³è¿‡æ‰€æœ‰ç¼“å­˜å¤„ç†ï¼š
```python
if cached_content is not None:
    return messages, optional_params, cached_content
```

**åŒæ­¥ç‰ˆæœ¬**ï¼š
```python
def sync_transform_request_body(...):
    from ..context_caching.vertex_ai_context_caching import ContextCachingEndpoints
    
    context_caching_endpoints = ContextCachingEndpoints()
    
    messages, optional_params, cached_content = context_caching_endpoints.check_and_create_cache(
        messages=messages,
        optional_params=optional_params,
        api_key=gemini_api_key or "dummy",
        api_base=api_base,
        model=model,
        client=client,
        timeout=timeout,
        extra_headers=extra_headers,
        cached_content=optional_params.pop("cached_content", None),
        logging_obj=logging_obj,
        custom_llm_provider=custom_llm_provider,
        vertex_project=vertex_project,
        vertex_location=vertex_location,
        vertex_auth_header=vertex_auth_header,
    )
```

**å¼‚æ­¥ç‰ˆæœ¬**ï¼š
```python
async def async_transform_request_body(...):
    from ..context_caching.vertex_ai_context_caching import ContextCachingEndpoints
    
    context_caching_endpoints = ContextCachingEndpoints()
    
    messages, optional_params, cached_content = await context_caching_endpoints.async_check_and_create_cache(
        messages=messages,
        ...
    )
```

**å…³é”®ç‚¹**ï¼š
- åœ¨è¯·æ±‚ä½“è½¬æ¢ä¹‹å‰æ‰§è¡Œ
- è¿”å›å¤„ç†åçš„ `messages`ï¼ˆå·²ç§»é™¤éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯ï¼‰
- è¿”å› `cached_content`ï¼ˆç¼“å­˜ IDï¼Œå¦‚æœæœ‰ï¼‰

### 2. æ¶ˆæ¯åˆ†ç¦»

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/transformation.py`

é¦–å…ˆåˆ†ç¦»éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯ï¼š

**è¾¹ç•Œæƒ…å†µ**ï¼šå¦‚æœæ²¡æœ‰éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›ï¼š
```python
if len(cached_messages) == 0:
    return messages, optional_params, None
```

```python
def separate_cached_messages(
    messages: List[AllMessageValues],
) -> Tuple[List[AllMessageValues], List[AllMessageValues]]:
    """
    åˆ†ç¦»å¸¦ cache_control æ ‡è®°çš„æ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯
    
    è¦æ±‚ï¼šç¼“å­˜æ¶ˆæ¯å¿…é¡»æ˜¯è¿ç»­çš„å—ï¼ˆä¸èƒ½åˆ†æ•£ï¼‰
    """
    cached_messages: List[AllMessageValues] = []
    non_cached_messages: List[AllMessageValues] = []

    # æå–å¸¦ cache_control æ ‡è®°çš„æ¶ˆæ¯åŠå…¶ç´¢å¼•
    filtered_messages: List[Tuple[int, AllMessageValues]] = []
    for idx, message in enumerate(messages):
        if is_cached_message(message=message):  # æ£€æŸ¥æ˜¯å¦æœ‰ cache_control æ ‡è®°
            filtered_messages.append((idx, message))

    # éªŒè¯ç¼“å­˜æ¶ˆæ¯å¿…é¡»æ˜¯è¿ç»­çš„å—
    last_continuous_block_idx = get_first_continuous_block_idx(filtered_messages)
    
    if filtered_messages and last_continuous_block_idx is not None:
        first_cached_idx = filtered_messages[0][0]
        last_cached_idx = filtered_messages[last_continuous_block_idx][0]

        cached_messages = messages[first_cached_idx : last_cached_idx + 1]
        non_cached_messages = (
            messages[:first_cached_idx] + messages[last_cached_idx + 1 :]
        )
    else:
        non_cached_messages = messages

    return cached_messages, non_cached_messages
```

**`is_cached_message` å‡½æ•°**ï¼ˆæ¥è‡ª `litellm/utils.py`ï¼‰ï¼š
- æ£€æŸ¥æ¶ˆæ¯çš„ `content` å­—æ®µä¸­æ˜¯å¦åŒ…å« `cache_control` æ ‡è®°
- `cache_control` å¿…é¡»åŒ…å« `"type": "ephemeral"`

**é€»è¾‘è¯´æ˜**ï¼š
- è¯†åˆ«å¸¦æœ‰ `cache_control` æ ‡è®°çš„æ¶ˆæ¯
- è¦æ±‚ç¼“å­˜æ¶ˆæ¯å¿…é¡»æ˜¯è¿ç»­çš„å—ï¼ˆä¸èƒ½åˆ†æ•£ï¼‰
- è¿”å›åˆ†ç¦»åçš„ä¸¤éƒ¨åˆ†æ¶ˆæ¯

### 3. ç”Ÿæˆç¼“å­˜é”®

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

åŸºäºæ¶ˆæ¯å†…å®¹å’Œå·¥å…·å®šä¹‰ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®ï¼š

```python
## Generate cache key
generated_cache_key = local_cache_obj.get_cache_key(
    messages=cached_messages, tools=tools
)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `Cache.get_cache_key()` ç”Ÿæˆå“ˆå¸Œé”®
- åŒ…å« `cached_messages` å’Œ `tools` çš„å†…å®¹
- ç›¸åŒå†…å®¹ä¼šç”Ÿæˆç›¸åŒçš„é”®
- ç”¨äºåœ¨ Google API ä¸­æŸ¥æ‰¾ç¼“å­˜ï¼ˆé€šè¿‡ `displayName` å­—æ®µï¼‰

### 4. å·¥å…·ï¼ˆToolsï¼‰å¤„ç†

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

åœ¨ç”Ÿæˆç¼“å­˜é”®ä¹‹å‰ï¼Œä¼šä» `optional_params` ä¸­æå– `tools`ï¼š

```python
tools = optional_params.pop("tools", None)
```

**è¯´æ˜**ï¼š
- `tools` ä¼šè¢«åŒ…å«åœ¨ç¼“å­˜é”®çš„ç”Ÿæˆä¸­
- å¦‚æœç¼“å­˜ä¸­åŒ…å« `tools`ï¼Œåˆ›å»ºç¼“å­˜æ—¶ä¹Ÿä¼šåŒ…å« `tools`
- æå–åä» `optional_params` ä¸­ç§»é™¤ï¼Œé¿å…é‡å¤å¤„ç†

### 5. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆä¼˜åŒ–æ­¥éª¤ï¼‰

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

é¦–å…ˆæ£€æŸ¥æœ¬åœ°å†…å­˜ç¼“å­˜ï¼Œé¿å…ç½‘ç»œè¯·æ±‚ï¼š

```python
# OPTIMIZATION: Check local cache first (no network call, with project/location scope)
local_cache_id = self.local_cache_manager.get_cache(
    cache_key=generated_cache_key,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
    custom_llm_provider=custom_llm_provider
)
if local_cache_id is not None:
    # Found valid cache locally, return immediately
    return non_cached_messages, optional_params, local_cache_id
```

**è¯´æ˜**ï¼š
- ä½¿ç”¨ä½œç”¨åŸŸé”®ï¼ˆåŒ…å« project:locationï¼‰æŸ¥æ‰¾
- å¦‚æœæ‰¾åˆ°ä¸”æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›ï¼Œæ— éœ€ç½‘ç»œè¯·æ±‚
- è¿™æ˜¯æ€§èƒ½ä¼˜åŒ–çš„å…³é”®æ­¥éª¤

### 6. æ£€æŸ¥ Google API ç¼“å­˜

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

å¦‚æœæœ¬åœ°ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢ Google APIï¼š

```python
def check_cache(
    self,
    cache_key: str,
    client: HTTPHandler,  # æˆ– AsyncHTTPHandler
    headers: dict,
    ...
) -> Optional[str]:
    """
    æŸ¥è¯¢ Google API æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
    
    è¿”å›:
        - cache_id: å¦‚æœæ‰¾åˆ°ç¼“å­˜
        - None: å¦‚æœæœªæ‰¾åˆ°æˆ–å·²è¿‡æœŸ
    """
    # æ„é€  API URL
    _, url = self._get_token_and_url_context_caching(...)
    
    # GET è¯·æ±‚åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
    resp = client.get(url=url, headers=headers)
    raw_response = resp.json()
    
    # æŸ¥æ‰¾ displayName == cache_key çš„ç¼“å­˜
    for cached_item in raw_response["cachedContents"]:
        display_name = cached_item.get("displayName")
        if display_name is not None and display_name == cache_key:
            cache_id = cached_item.get("name")
            expire_time = cached_item.get("expireTime")
            
            if cache_id:
                # è®¡ç®—å‰©ä½™ TTL
                if expire_time:
                    remaining_ttl = parse_expire_time_to_remaining_ttl(expire_time)
                    if remaining_ttl is None:
                        # å·²è¿‡æœŸï¼Œä¸å­˜å…¥æœ¬åœ°ç¼“å­˜
                        return None
                    ttl_seconds = remaining_ttl
                else:
                    ttl_seconds = 3600.0
                
                # å­˜å…¥æœ¬åœ°ç¼“å­˜ï¼ˆä¸‹æ¬¡å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼‰
                self.local_cache_manager.set_cache(
                    cache_key=cache_key,
                    cache_id=cache_id,
                    ttl_seconds=ttl_seconds,
                    vertex_project=vertex_project,
                    vertex_location=vertex_location,
                    custom_llm_provider=custom_llm_provider
                )
                
            return cache_id
    
    return None
```

**é€»è¾‘è¯´æ˜**ï¼š
- è°ƒç”¨ `GET /cachedContents` åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
- éå†æŸ¥æ‰¾ `displayName == cache_key` çš„ç¼“å­˜
- å¦‚æœæ‰¾åˆ°ï¼Œä» `expireTime` è®¡ç®—å‰©ä½™ TTL
- å¦‚æœå·²è¿‡æœŸï¼Œè¿”å› `None`ï¼Œä¸å­˜å…¥æœ¬åœ°ç¼“å­˜
- å¦‚æœæœªè¿‡æœŸï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜å¹¶è¿”å› `cache_id`

### 7. åˆ›å»ºæ–°ç¼“å­˜

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py`

å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç¼“å­˜ï¼š

```python
## TRANSFORM REQUEST
cached_content_request_body = transform_openai_messages_to_gemini_context_caching(
    model=model,
    messages=cached_messages,
    cache_key=generated_cache_key,
    custom_llm_provider=custom_llm_provider,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
)

cached_content_request_body["tools"] = tools

# POST è¯·æ±‚åˆ›å»ºç¼“å­˜
response = client.post(url=url, headers=headers, json=cached_content_request_body)
raw_response_cached = response.json()

cache_id = raw_response_cached["name"]

# å­˜å…¥æœ¬åœ°ç¼“å­˜
ttl_str = cached_content_request_body.get("ttl")
if ttl_str:
    ttl_seconds = parse_ttl_to_seconds(ttl_str)
else:
    ttl_str_from_messages = extract_ttl_from_cached_messages(cached_messages)
    ttl_seconds = parse_ttl_to_seconds(ttl_str_from_messages) if ttl_str_from_messages else 3600.0

self.local_cache_manager.set_cache(
    cache_key=generated_cache_key,
    cache_id=cache_id,
    ttl_seconds=ttl_seconds,
    vertex_project=vertex_project,
    vertex_location=vertex_location,
    custom_llm_provider=custom_llm_provider
)

return non_cached_messages, optional_params, cache_id
```

**æ­¥éª¤**ï¼š
1. è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Gemini æ ¼å¼
2. æ·»åŠ  `displayName`ï¼ˆå³ `cache_key`ï¼‰
3. æå– TTLï¼ˆä»æ¶ˆæ¯æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
4. è°ƒç”¨ `POST /cachedContents` åˆ›å»ºç¼“å­˜
5. å­˜å…¥æœ¬åœ°ç¼“å­˜ï¼ˆåŒ…å« TTLï¼‰
6. è¿”å›æ–°åˆ›å»ºçš„ `cache_id`

### 8. æ¶ˆæ¯æ ¼å¼è½¬æ¢

**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/transformation.py`

å°† OpenAI æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º Gemini ç¼“å­˜è¯·æ±‚æ ¼å¼ï¼š

```python
def transform_openai_messages_to_gemini_context_caching(
    model: str,
    messages: List[AllMessageValues],
    custom_llm_provider: Literal["vertex_ai", "vertex_ai_beta", "gemini"],
    cache_key: str,
    vertex_project: Optional[str],
    vertex_location: Optional[str],
) -> CachedContentRequestBody:
    # æå– TTLï¼ˆåœ¨ç³»ç»Ÿæ¶ˆæ¯è½¬æ¢ä¹‹å‰ï¼‰
    ttl = extract_ttl_from_cached_messages(messages)
    
    # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯
    supports_system_message = get_supports_system_message(...)
    transformed_system_messages, new_messages = _transform_system_message(...)
    
    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    transformed_messages = _gemini_convert_messages_with_history(messages=new_messages, model=model)
    
    # æ„é€ æ¨¡å‹åç§°
    model_name = "models/{}".format(model)
    if custom_llm_provider == "vertex_ai" or custom_llm_provider == "vertex_ai_beta":
        model_name = f"projects/{vertex_project}/locations/{vertex_location}/publishers/google/{model_name}"

    data = CachedContentRequestBody(
        contents=transformed_messages,
        model=model_name,
        displayName=cache_key,
    )
    
    # æ·»åŠ  TTLï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
    if ttl:
        data["ttl"] = ttl
    
    # æ·»åŠ ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if transformed_system_messages is not None:
        data["system_instruction"] = transformed_system_messages

    return data
```

**å…³é”®è½¬æ¢**ï¼š
- OpenAI æ¶ˆæ¯æ ¼å¼ â†’ Gemini æ¶ˆæ¯æ ¼å¼
- æå–ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
- æå– TTLï¼ˆä» `cache_control.ttl`ï¼‰
- æ„é€ å®Œæ•´çš„æ¨¡å‹è·¯å¾„ï¼ˆVertex AI vs Google AI Studioï¼‰

### 9. ä½¿ç”¨ç¼“å­˜å‘èµ·è¯·æ±‚

**æ–‡ä»¶**: `litellm/llms/vertex_ai/gemini/transformation.py`

ç¼“å­˜å¤„ç†å®Œæˆåï¼Œåœ¨è¯·æ±‚ä½“ä¸­æ·»åŠ  `cachedContent` å­—æ®µï¼š

```python
def _transform_request_body(
    messages: List[AllMessageValues],
    model: str,
    optional_params: dict,
    custom_llm_provider: Literal["vertex_ai", "vertex_ai_beta", "gemini"],
    litellm_params: dict,
    cached_content: Optional[str],  # ç¼“å­˜ ID
) -> RequestBody:
    # ... å…¶ä»–è½¬æ¢é€»è¾‘ ...
    
    # å¦‚æœ cached_content ä¸ä¸ºç©ºï¼Œæ·»åŠ åˆ°è¯·æ±‚ä½“
    if cached_content:
        data["cachedContent"] = cached_content
    
    return data
```

**è¯´æ˜**ï¼š
- `cachedContent` å­—æ®µæŒ‡å®šè¦ä½¿ç”¨çš„ç¼“å­˜ ID
- Google ä¼šè‡ªåŠ¨ä»ç¼“å­˜åŠ è½½ä¹‹å‰çš„ä¸Šä¸‹æ–‡
- `contents` ä¸­åªéœ€è¦åŒ…å«æ–°çš„æ¶ˆæ¯ï¼ˆ`non_cached_messages`ï¼‰

---

## å…³é”®ç»„ä»¶è¯¦è§£

### 1. ContextCachingEndpoints ç±»

**èŒè´£**ï¼šç®¡ç†ç¼“å­˜çš„æ£€æŸ¥ã€åˆ›å»ºå’ŒæŸ¥è¯¢

**æ ¸å¿ƒæ–¹æ³•**ï¼š

| æ–¹æ³• | è¯´æ˜ | åŒæ­¥/å¼‚æ­¥ |
|------|------|----------|
| `check_and_create_cache()` | æ£€æŸ¥å¹¶åˆ›å»ºç¼“å­˜ï¼ˆä¸»å…¥å£ï¼‰ | åŒæ­¥ |
| `async_check_and_create_cache()` | å¼‚æ­¥ç‰ˆæœ¬ | å¼‚æ­¥ |
| `check_cache()` | æŸ¥è¯¢ Google API æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ | åŒæ­¥ |
| `async_check_cache()` | å¼‚æ­¥ç‰ˆæœ¬ | å¼‚æ­¥ |
| `_get_token_and_url_context_caching()` | æ„é€  API URL å’Œè®¤è¯ä¿¡æ¯ | å†…éƒ¨æ–¹æ³• |

**URL æ„é€ é€»è¾‘**ï¼š
- **Google AI Studio** (`gemini`): `https://generativelanguage.googleapis.com/v1beta/cachedContents?key={API_KEY}`
- **Vertex AI** (`vertex_ai`): 
  - `global` åŒºåŸŸ: `https://aiplatform.googleapis.com/v1/projects/{PROJECT}/locations/global/cachedContents`
  - å…¶ä»–åŒºåŸŸ: `https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT}/locations/{LOCATION}/cachedContents`
- **Vertex AI Beta** (`vertex_ai_beta`): ä½¿ç”¨ `v1beta1` API ç‰ˆæœ¬

**åˆå§‹åŒ–**ï¼š
```python
def __init__(self) -> None:
    self.local_cache_manager = get_cache_manager()  # æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨
```

### 2. LocalCacheManager ç±»

**èŒè´£**ï¼šæœ¬åœ°å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤ç½‘ç»œè¯·æ±‚

**æ ¸å¿ƒæ•°æ®ç»“æ„**ï¼š

```python
class CacheEntry:
    cache_id: str           # Google è¿”å›çš„ç¼“å­˜ ID
    created_at: float       # åˆ›å»ºæ—¶é—´æˆ³
    ttl_seconds: float      # è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    expire_time: float      # è¿‡æœŸæ—¶é—´æˆ³

class LocalCacheManager:
    _cache: Dict[str, CacheEntry]  # ç¼“å­˜å­˜å‚¨
    _lock: threading.Lock           # çº¿ç¨‹å®‰å…¨é”
```

**å…³é”®æ–¹æ³•**ï¼š

1. **`set_cache()`**ï¼šå­˜å‚¨ç¼“å­˜æ˜ å°„
   - ä½¿ç”¨ä½œç”¨åŸŸé”®ï¼ˆåŒ…å« project:locationï¼‰
   - TTL å‡å» 5 ç§’ç¼“å†²ï¼Œé¿å…è¾¹ç•Œæƒ…å†µ

2. **`get_cache()`**ï¼šè·å–ç¼“å­˜ ID
   - æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
   - å¦‚æœè¿‡æœŸï¼Œè‡ªåŠ¨æ¸…ç†å¹¶è¿”å› `None`

3. **`_make_scoped_key()`**ï¼šç”Ÿæˆä½œç”¨åŸŸé”®
   - Google AI Studio (`gemini`) ä¸éœ€è¦é¡¹ç›®ä½œç”¨åŸŸ
   - Vertex AI ä½¿ç”¨ `project:location` ä½œä¸ºä½œç”¨åŸŸ
   - ç¡®ä¿ä¸åŒé¡¹ç›®çš„ç¼“å­˜ä¸ä¼šå†²çª

### 3. Transformation æ¨¡å—

**èŒè´£**ï¼šæ¶ˆæ¯æ ¼å¼è½¬æ¢å’Œç¼“å­˜æ¶ˆæ¯è¯†åˆ«

**å…³é”®å‡½æ•°**ï¼š

1. **`separate_cached_messages()`**ï¼šåˆ†ç¦»ç¼“å­˜æ¶ˆæ¯
   - è¯†åˆ«å¸¦ `cache_control` æ ‡è®°çš„æ¶ˆæ¯
   - è¦æ±‚ç¼“å­˜æ¶ˆæ¯å¿…é¡»æ˜¯è¿ç»­çš„å—

2. **`transform_openai_messages_to_gemini_context_caching()`**ï¼šæ ¼å¼è½¬æ¢
   - OpenAI æ ¼å¼ â†’ Gemini æ ¼å¼
   - æå–ç³»ç»Ÿæ¶ˆæ¯å’Œ TTL

3. **`extract_ttl_from_cached_messages()`**ï¼šæå– TTL
   - ä» `cache_control.ttl` æå–
   - éªŒè¯æ ¼å¼æœ‰æ•ˆæ€§

4. **`get_first_continuous_block_idx()`**ï¼šéªŒè¯ç¼“å­˜æ¶ˆæ¯è¿ç»­æ€§
   - ç¡®ä¿ç¼“å­˜æ¶ˆæ¯æ˜¯è¿ç»­çš„å—

### 4. è¾…åŠ©å‡½æ•°

**`parse_ttl_to_seconds()`**ï¼š
- è§£æ TTL å­—ç¬¦ä¸²ï¼ˆå¦‚ "3600s"ï¼‰ä¸ºç§’æ•°
- é»˜è®¤å€¼ï¼š3600.0 ç§’ï¼ˆ1 å°æ—¶ï¼‰

**`parse_expire_time_to_remaining_ttl()`**ï¼š
- è§£æ ISO 8601 æ ¼å¼çš„ `expireTime`
- è®¡ç®—å‰©ä½™ TTLï¼ˆç§’ï¼‰
- å¦‚æœå·²è¿‡æœŸï¼Œè¿”å› `None`

---

## æ•°æ®æµè½¬

### å®Œæ•´æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥ (OpenAI æ ¼å¼)
    â†“
messages = [
    {
        "role": "system",
        "content": [{
            "type": "text",
            "text": "...",
            "cache_control": {"type": "ephemeral", "ttl": "3600s"}
        }]
    },
    {"role": "user", "content": "é—®é¢˜"}
]
    â†“
separate_cached_messages()
    â†“
cached_messages = [messages[0]]
non_cached_messages = [messages[1]]
    â†“
ç”Ÿæˆ cache_key = hash(cached_messages + tools)
    â†“
æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    â”œâ”€ å‘½ä¸­ â†’ è¿”å› cache_idï¼ˆæ— éœ€ç½‘ç»œè¯·æ±‚ï¼‰
    â””â”€ æœªå‘½ä¸­ â†’ ç»§ç»­
    â†“
æ£€æŸ¥ Google API (GET /cachedContents)
    â”œâ”€ æ‰¾åˆ° â†’ è®¡ç®—å‰©ä½™ TTL â†’ å­˜å…¥æœ¬åœ°ç¼“å­˜ â†’ è¿”å› cache_id
    â””â”€ æœªæ‰¾åˆ° â†’ ç»§ç»­
    â†“
åˆ›å»ºæ–°ç¼“å­˜ (POST /cachedContents)
    â†“
æå– TTL â†’ å­˜å…¥æœ¬åœ°ç¼“å­˜ â†’ è¿”å› cache_id
    â†“
æ„é€  Gemini è¯·æ±‚ä½“
    {
        "contents": transform(non_cached_messages),
        "cachedContent": cache_id
    }
    â†“
å‘é€åˆ° Google API
```

### å…³é”®æ•°æ®ç»“æ„

**è¾“å…¥**ï¼š
```python
messages: List[AllMessageValues]  # OpenAI æ ¼å¼æ¶ˆæ¯
optional_params: dict             # åŒ…å« tools ç­‰
```

**è¾“å‡º**ï¼š
```python
non_cached_messages: List[AllMessageValues]  # å¤„ç†åçš„æ¶ˆæ¯
optional_params: dict                         # æ›´æ–°åçš„å‚æ•°
cached_content: Optional[str]                 # ç¼“å­˜ ID
```

**ç¼“å­˜é”®æ ¼å¼**ï¼š
```
åŸºç¡€ç‰ˆæœ¬: "cache-key-hash-xxx"
ä½œç”¨åŸŸç‰ˆæœ¬: "cache-key-hash-xxx:project:location:scope-hash"
```

**ç¼“å­˜ ID æ ¼å¼**ï¼š
```
Vertex AI: "projects/{project}/locations/{location}/cachedContents/{id}"
Google AI Studio: "cachedContents/{id}"
```

---

## ä¼˜åŒ–æœºåˆ¶

### æœ¬åœ°ç¼“å­˜ä¼˜åŒ–

**é—®é¢˜**ï¼šæ¯æ¬¡è¯·æ±‚éƒ½è°ƒç”¨ Google API æ£€æŸ¥ç¼“å­˜ï¼Œå¢åŠ å»¶è¿Ÿå’Œç½‘ç»œå¼€é”€

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨å†…å­˜ä¸­ç¼“å­˜ `cache_key â†’ cache_id` æ˜ å°„

**ä¼˜åŒ–æµç¨‹**ï¼š

```python
def check_and_create_cache(self, ...):
    # 1. ç”Ÿæˆç¼“å­˜é”®
    generated_cache_key = local_cache_obj.get_cache_key(...)
    
    # 2. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆæ— ç½‘ç»œè¯·æ±‚ï¼‰
    local_cache_id = self.local_cache_manager.get_cache(
        cache_key=generated_cache_key,
        vertex_project=vertex_project,
        vertex_location=vertex_location,
        custom_llm_provider=custom_llm_provider
    )
    if local_cache_id is not None:
        return non_cached_messages, optional_params, local_cache_id
    
    # 3. æœ¬åœ°æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢ Google API
    google_cache_name = self.check_cache(...)
    
    # 4. å¦‚æœæ‰¾åˆ°ï¼Œcheck_cache ä¼šè‡ªåŠ¨å­˜å…¥æœ¬åœ°ç¼“å­˜
    
    # 5. å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ›å»ºæ–°ç¼“å­˜å¹¶å­˜å…¥æœ¬åœ°ç¼“å­˜
```

**æ€§èƒ½æå‡**ï¼š

| åœºæ™¯ | åŸå§‹å®ç° | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡ |
|------|---------|---------|------|
| é¦–æ¬¡è¯·æ±‚ | 1.5ç§’ | 1.5ç§’ | 0% |
| ç¼“å­˜å‘½ä¸­ | 0.8ç§’ | 0.3ç§’ | **62% â†“** |
| ç½‘ç»œè°ƒç”¨ | æ¯æ¬¡æ£€æŸ¥ | ä»…æœªå‘½ä¸­æ—¶ | **60-80% â†“** |

### TTL ç®¡ç†

**TTL æå–**ï¼š
- ä»æ¶ˆæ¯çš„ `cache_control.ttl` æå–
- æ ¼å¼ï¼š`"3600s"`ï¼ˆå­—ç¬¦ä¸²ï¼Œä»¥ 's' ç»“å°¾ï¼‰
- éªŒè¯æ ¼å¼æœ‰æ•ˆæ€§

**TTL è®¡ç®—**ï¼š
- åˆ›å»ºç¼“å­˜æ—¶ï¼šä½¿ç”¨æ¶ˆæ¯ä¸­çš„ TTL æˆ–é»˜è®¤å€¼ï¼ˆ3600 ç§’ï¼‰
- æŸ¥è¯¢ç¼“å­˜æ—¶ï¼šä» `expireTime` è®¡ç®—å‰©ä½™ TTL
- æœ¬åœ°ç¼“å­˜ï¼šTTL å‡å» 5 ç§’ç¼“å†²ï¼Œé¿å…è¾¹ç•Œæƒ…å†µ

**è¿‡æœŸå¤„ç†**ï¼š
- æœ¬åœ°ç¼“å­˜ï¼šè‡ªåŠ¨æ£€æŸ¥è¿‡æœŸï¼Œè¿‡æœŸåè‡ªåŠ¨æ¸…ç†
- Google ç¼“å­˜ï¼šå¦‚æœ `expireTime` å·²è¿‡æœŸï¼Œä¸å­˜å…¥æœ¬åœ°ç¼“å­˜

---

## åŒæ­¥ä¸å¼‚æ­¥ç‰ˆæœ¬

### ä½¿ç”¨åœºæ™¯

**åŒæ­¥ç‰ˆæœ¬** (`check_and_create_cache`):
- ç”¨æˆ·è°ƒç”¨ `litellm.completion()`ï¼ˆåŒæ­¥ APIï¼‰
- åœ¨åŒæ­¥ä»£ç ä¸­è°ƒç”¨
- ç®€å•è„šæœ¬æˆ–å•çº¿ç¨‹åº”ç”¨

**å¼‚æ­¥ç‰ˆæœ¬** (`async_check_and_create_cache`):
- ç”¨æˆ·è°ƒç”¨ `litellm.acompletion()`ï¼ˆå¼‚æ­¥ APIï¼‰
- åœ¨å¼‚æ­¥ä»£ç ä¸­è°ƒç”¨ï¼ˆ`async def` å‡½æ•°å†…ï¼‰
- éœ€è¦å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
- åœ¨å¼‚æ­¥æ¡†æ¶ä¸­ä½¿ç”¨ï¼ˆå¦‚ FastAPIã€aiohttp ç­‰ï¼‰

### å®ç°å·®å¼‚

| ç‰¹æ€§ | åŒæ­¥ç‰ˆæœ¬ | å¼‚æ­¥ç‰ˆæœ¬ |
|------|---------|---------|
| HTTP å®¢æˆ·ç«¯ | `HTTPHandler` | `AsyncHTTPHandler` |
| è°ƒç”¨æ–¹å¼ | é˜»å¡ç­‰å¾… | éé˜»å¡ï¼Œä½¿ç”¨ `await` |
| å¹¶å‘èƒ½åŠ› | é¡ºåºå¤„ç† | å¯å¹¶å‘å¤„ç† |
| æœ¬åœ°ç¼“å­˜ | å…±äº«åŒä¸€ä¸ª `LocalCacheManager` | å…±äº«åŒä¸€ä¸ª `LocalCacheManager` |

### é‡è¦æç¤º

1. **æœ¬åœ°ç¼“å­˜æ˜¯å…±äº«çš„**ï¼šåŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬å…±äº«åŒä¸€ä¸ªæœ¬åœ°ç¼“å­˜ç®¡ç†å™¨
2. **ä¸è¦æ··ç”¨**ï¼šä¸è¦åœ¨å¼‚æ­¥å‡½æ•°ä¸­è°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œä¼šé˜»å¡äº‹ä»¶å¾ªç¯
3. **è‡ªåŠ¨é€‰æ‹©**ï¼šæ ¹æ®ä½ è°ƒç”¨çš„ APIï¼ˆ`completion` vs `acompletion`ï¼‰è‡ªåŠ¨é€‰æ‹©å¯¹åº”ç‰ˆæœ¬

---

## é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ

### é”™è¯¯å¤„ç†

1. **ç½‘ç»œé”™è¯¯**ï¼š
   - `httpx.HTTPStatusError`ï¼šè½¬æ¢ä¸º `VertexAIError`
   - `httpx.TimeoutException`ï¼šè¿”å› 408 é”™è¯¯
   - 403 é”™è¯¯ï¼šè¿”å› `None`ï¼ˆæƒé™ä¸è¶³ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼‰

2. **ç¼“å­˜æŸ¥è¯¢å¤±è´¥**ï¼š
   - å¦‚æœæŸ¥è¯¢ Google API å¤±è´¥ï¼Œè¿”å› `None`
   - ä¸ä¼šå½±å“ä¸»æµç¨‹ï¼Œä¼šå°è¯•åˆ›å»ºæ–°ç¼“å­˜

3. **ç¼“å­˜åˆ›å»ºå¤±è´¥**ï¼š
   - æŠ›å‡º `VertexAIError`ï¼ŒåŒ…å«é”™è¯¯ç å’Œé”™è¯¯ä¿¡æ¯

### è¾¹ç•Œæƒ…å†µ

1. **å·²æœ‰ cached_content å‚æ•°**ï¼š
   - å¦‚æœ `optional_params` ä¸­å·²æœ‰ `cached_content`ï¼Œç›´æ¥ä½¿ç”¨ï¼Œè·³è¿‡æ‰€æœ‰ç¼“å­˜å¤„ç†
   - é€‚ç”¨äºæ‰‹åŠ¨æŒ‡å®šç¼“å­˜ ID çš„åœºæ™¯

2. **æ²¡æœ‰ç¼“å­˜æ¶ˆæ¯**ï¼š
   - å¦‚æœ `cached_messages` ä¸ºç©ºï¼Œç›´æ¥è¿”å›ï¼Œä¸è¿›è¡Œç¼“å­˜å¤„ç†
   - è¿”å›åŸå§‹çš„ `messages` å’Œ `None` ä½œä¸º `cached_content`

3. **ç¼“å­˜æ¶ˆæ¯ä¸è¿ç»­**ï¼š
   - `separate_cached_messages` è¦æ±‚ç¼“å­˜æ¶ˆæ¯å¿…é¡»æ˜¯è¿ç»­çš„å—
   - å¦‚æœä¸è¿ç»­ï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ªè¿ç»­å—
   - åç»­çš„ç¼“å­˜æ¶ˆæ¯ä¼šè¢«å½“ä½œæ™®é€šæ¶ˆæ¯å¤„ç†

4. **TTL æ— æ•ˆ**ï¼š
   - å¦‚æœ TTL æ ¼å¼æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆ3600 ç§’ï¼‰
   - TTL æ ¼å¼å¿…é¡»æ˜¯ `"æ•°å­—s"`ï¼Œå¦‚ `"3600s"`

5. **expireTime è§£æå¤±è´¥**ï¼š
   - å¦‚æœæ— æ³•è§£æ `expireTime`ï¼Œä½¿ç”¨é»˜è®¤ TTLï¼ˆ3600 ç§’ï¼‰
   - è§£æå¤±è´¥ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ä½¿ç”¨é»˜è®¤å€¼

6. **æœ¬åœ°ç¼“å­˜è¿‡æœŸ**ï¼š
   - è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ¡ç›®
   - è¿”å› `None`ï¼Œç»§ç»­æŸ¥è¯¢ Google API
   - è¿‡æœŸæ£€æŸ¥åœ¨ `get_cache()` æ—¶è‡ªåŠ¨è¿›è¡Œ

7. **Google ç¼“å­˜å·²è¿‡æœŸ**ï¼š
   - å¦‚æœ `expireTime` å·²è¿‡æœŸï¼Œä¸å­˜å…¥æœ¬åœ°ç¼“å­˜
   - è¿”å› `None`ï¼Œä¼šåˆ›å»ºæ–°ç¼“å­˜

8. **ç½‘ç»œè¯·æ±‚å¤±è´¥**ï¼š
   - 403 é”™è¯¯ï¼šè¿”å› `None`ï¼ˆæƒé™ä¸è¶³ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
   - å…¶ä»– HTTP é”™è¯¯ï¼šæŠ›å‡º `VertexAIError`
   - è¶…æ—¶ï¼šæŠ›å‡º `VertexAIError`ï¼ˆ408 çŠ¶æ€ç ï¼‰

9. **å·¥å…·ï¼ˆToolsï¼‰å¤„ç†**ï¼š
   - `tools` ä¼šè¢«åŒ…å«åœ¨ç¼“å­˜é”®ä¸­
   - å¦‚æœæ¶ˆæ¯ç›¸åŒä½† `tools` ä¸åŒï¼Œä¼šç”Ÿæˆä¸åŒçš„ç¼“å­˜é”®
   - åˆ›å»ºç¼“å­˜æ—¶ï¼Œ`tools` ä¼šè¢«åŒ…å«åœ¨è¯·æ±‚ä½“ä¸­

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [{
            "type": "text",
            "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹ã€‚",
            "cache_control": {
                "type": "ephemeral",
                "ttl": "3600s"  # ç¼“å­˜ 1 å°æ—¶
            }
        }]
    },
    {
        "role": "user",
        "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ Vertex AIï¼Ÿ"
    }
]

# ç¬¬ä¸€æ¬¡è°ƒç”¨ - åˆ›å»ºç¼“å­˜
response = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,
    vertex_project="my-project",
    vertex_location="global"
)

# ç¬¬äºŒæ¬¡è°ƒç”¨ - ä½¿ç”¨ç¼“å­˜ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
response2 = completion(
    model="vertex_ai/gemini-2.0-flash-001",
    messages=messages,  # ç›¸åŒçš„ç¼“å­˜å†…å®¹
    vertex_project="my-project",
    vertex_location="global"
)
```

### å¼‚æ­¥ä½¿ç”¨

```python
import asyncio
from litellm import acompletion

async def main():
    messages = [
        {
            "role": "system",
            "content": [{
                "type": "text",
                "text": "ç³»ç»Ÿæç¤ºè¯...",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }]
        },
        {"role": "user", "content": "é—®é¢˜"}
    ]

    response = await acompletion(
        model="vertex_ai/gemini-2.0-flash-001",
        messages=messages,
        vertex_project="my-project",
        vertex_location="global"
    )
    
    print(response.choices[0].message.content)

asyncio.run(main())
```

### ç›‘æ§ç¼“å­˜

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

# è·å–ç¼“å­˜ç®¡ç†å™¨
cache_manager = get_cache_manager()

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
stats = cache_manager.get_stats()
print(f"æ€»ç¼“å­˜æ¡ç›®: {stats['total_entries']}")
print(f"æœ‰æ•ˆæ¡ç›®: {stats['valid_entries']}")
print(f"è¿‡æœŸæ¡ç›®: {stats['expired_entries']}")

# æ¸…ç†è¿‡æœŸç¼“å­˜
removed = cache_manager.cleanup_expired()
print(f"æ¸…ç†äº† {removed} ä¸ªè¿‡æœŸæ¡ç›®")
```

---

## æ€»ç»“

### æ ¸å¿ƒè®¾è®¡æ€æƒ³

1. **åˆ†ç¦»å…³æ³¨ç‚¹**ï¼šç¼“å­˜é€»è¾‘ç‹¬ç«‹äºä¸»æµç¨‹
2. **é€æ˜é›†æˆ**ï¼šåœ¨æ¶ˆæ¯è½¬æ¢é˜¶æ®µè‡ªåŠ¨å¤„ç†
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šæœ¬åœ°ç¼“å­˜å‡å°‘ç½‘ç»œè¯·æ±‚
4. **å¤šé¡¹ç›®æ”¯æŒ**ï¼šé€šè¿‡ä½œç”¨åŸŸéš”ç¦»ä¸åŒé¡¹ç›®

### å…³é”®ç‰¹æ€§

- âœ… è‡ªåŠ¨è¯†åˆ«éœ€è¦ç¼“å­˜çš„æ¶ˆæ¯ï¼ˆé€šè¿‡ `cache_control` æ ‡è®°ï¼‰
- âœ… è‡ªåŠ¨æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ï¼ˆæœ¬åœ°ç¼“å­˜ + Google APIï¼‰
- âœ… è‡ªåŠ¨åˆ›å»ºæ–°ç¼“å­˜ï¼ˆå¦‚éœ€è¦ï¼‰
- âœ… æœ¬åœ°ç¼“å­˜ä¼˜åŒ–ï¼ˆå‡å°‘ 60-80% ç½‘ç»œè°ƒç”¨ï¼‰
- âœ… å¤šé¡¹ç›®éš”ç¦»ï¼ˆé€šè¿‡ project:location ä½œç”¨åŸŸï¼‰
- âœ… TTL ç®¡ç†ï¼ˆè‡ªåŠ¨è¿‡æœŸå’Œæ¸…ç†ï¼‰
- âœ… åŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬æ”¯æŒ

### ä½¿ç”¨å»ºè®®

1. **ç›´æ¥ä½¿ç”¨**ï¼š`ContextCachingEndpoints` å·²åŒ…å«æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
2. **å¤šé¡¹ç›®åœºæ™¯**ï¼šç¡®ä¿æ­£ç¡®ä¼ é€’ `vertex_project` å’Œ `vertex_location`
3. **ç›‘æ§ç¼“å­˜**ï¼šä½¿ç”¨ `LocalCacheManager.get_stats()` æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
4. **è‡ªåŠ¨ä¼˜åŒ–**ï¼šæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½å·²è‡ªåŠ¨å¯ç”¨ï¼Œæ— éœ€é¢å¤–é…ç½®
