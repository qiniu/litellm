# å¤šé¡¹ç›® Gemini Context Caching æ”¯æŒ

## é—®é¢˜åœºæ™¯

å½“æ‚¨çš„é¡¹ç›®é…ç½®äº†å¤šä¸ª Vertex AI é¡¹ç›®æ—¶ï¼Œéœ€è¦ç¡®ä¿ç¼“å­˜æ­£ç¡®éš”ç¦»ï¼š

```yaml
model_list:
  - model_name: gemini-2.0-flash-bz
    litellm_params:
      model: vertex_ai/gemini-2.0-flash-001
      vertex_project: "gemini-qn-bz"
      vertex_location: "global"

  - model_name: gemini-2.0-flash-prod
    litellm_params:
      model: vertex_ai/gemini-2.0-flash-001
      vertex_project: "gemini-prod"
      vertex_location: "global"

  - model_name: gemini-2.0-flash-dev
    litellm_params:
      model: vertex_ai/gemini-2.0-flash-001
      vertex_project: "gemini-dev"
      vertex_location: "us-central1"
```

**å…³é”®é—®é¢˜**ï¼šç›¸åŒçš„å†…å®¹åœ¨ä¸åŒé¡¹ç›®ä¸­ä¼šåˆ›å»ºç‹¬ç«‹çš„ç¼“å­˜ï¼Œå®ƒä»¬ä¸åº”è¯¥äº’ç›¸å½±å“ã€‚

## è§£å†³æ–¹æ¡ˆ

### ç¼“å­˜é”®ä½œç”¨åŸŸ

æˆ‘ä»¬çš„å®ç°ä½¿ç”¨ **é¡¹ç›® + åŒºåŸŸ** ä½œä¸ºç¼“å­˜é”®çš„ä½œç”¨åŸŸï¼š

```python
# ç¼“å­˜é”®æ ¼å¼
scoped_key = f"{cache_key}:{vertex_project}:{vertex_location}:{hash}"

# ç¤ºä¾‹
"content-hash:gemini-qn-bz:global:7c0ff9df"
"content-hash:gemini-prod:global:225155362"
"content-hash:gemini-dev:us-central1:8a3fe421"
```

### å·¥ä½œåŸç†

```
ç›¸åŒå†…å®¹ + ä¸åŒé¡¹ç›® = ç‹¬ç«‹ç¼“å­˜

content-hash-abc â†’ gemini-qn-bz    â†’ cache-id-1 âœ“
content-hash-abc â†’ gemini-prod     â†’ cache-id-2 âœ“
content-hash-abc â†’ gemini-dev      â†’ cache-id-3 âœ“
```

## å®é™…ä½¿ç”¨

### åœºæ™¯ 1: åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [{
            "type": "text",
            "text": "é•¿æ–‡æ¡£å†…å®¹...",
            "cache_control": {"type": "ephemeral", "ttl": "3600s"}
        }]
    },
    {"role": "user", "content": "é—®é¢˜"}
]

# ä½¿ç”¨ project-1
response1 = completion(model="gemini-2.0-flash-bz", messages=messages)
# æœ¬åœ°ç¼“å­˜é”®: content-hash:gemini-qn-bz:global:xxx

# ä½¿ç”¨ project-2ï¼ˆç›¸åŒå†…å®¹ï¼‰
response2 = completion(model="gemini-2.0-flash-prod", messages=messages)
# æœ¬åœ°ç¼“å­˜é”®: content-hash:gemini-prod:global:yyy

# âœ“ ä¸¤ä¸ªé¡¹ç›®å„è‡ªç»´æŠ¤ç‹¬ç«‹çš„ç¼“å­˜
# âœ“ ä¸ä¼šäº’ç›¸å¹²æ‰°
```

### åœºæ™¯ 2: æ‰‹åŠ¨ç®¡ç†ï¼ˆé«˜çº§ï¼‰

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

cache_manager = get_cache_manager()

# ä¸º project-1 è®¾ç½®ç¼“å­˜
cache_manager.set_cache(
    cache_key="content-hash-abc",
    cache_id="projects/gemini-qn-bz/locations/global/cachedContents/123",
    ttl_seconds=3600,
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)

# ä¸º project-2 è®¾ç½®ç¼“å­˜ï¼ˆç›¸åŒå†…å®¹ï¼‰
cache_manager.set_cache(
    cache_key="content-hash-abc",  # ç›¸åŒçš„å†…å®¹å“ˆå¸Œ
    cache_id="projects/gemini-prod/locations/global/cachedContents/456",
    ttl_seconds=3600,
    vertex_project="gemini-prod",  # ä¸åŒçš„é¡¹ç›®
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)

# è·å– project-1 çš„ç¼“å­˜
cache1 = cache_manager.get_cache(
    cache_key="content-hash-abc",
    vertex_project="gemini-qn-bz",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)
# è¿”å›: projects/gemini-qn-bz/locations/global/cachedContents/123

# è·å– project-2 çš„ç¼“å­˜
cache2 = cache_manager.get_cache(
    cache_key="content-hash-abc",
    vertex_project="gemini-prod",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)
# è¿”å›: projects/gemini-prod/locations/global/cachedContents/456
```

## ä½œç”¨åŸŸç»´åº¦

### 1. é¡¹ç›®éš”ç¦»

```python
# åŒä¸€å†…å®¹ï¼Œä¸åŒé¡¹ç›®
cache_key = "doc-hash-123"

# Project A
cache_manager.set_cache(
    cache_key, "cache-a", 3600,
    vertex_project="project-a",
    vertex_location="global"
)

# Project B
cache_manager.set_cache(
    cache_key, "cache-b", 3600,
    vertex_project="project-b",
    vertex_location="global"
)

# âœ“ ä¸¤ä¸ªç‹¬ç«‹çš„ç¼“å­˜
```

### 2. åŒºåŸŸéš”ç¦»

```python
# åŒä¸€é¡¹ç›®ï¼Œä¸åŒåŒºåŸŸ
cache_key = "doc-hash-123"

# Global region
cache_manager.set_cache(
    cache_key, "cache-global", 3600,
    vertex_project="my-project",
    vertex_location="global"
)

# US-Central1 region
cache_manager.set_cache(
    cache_key, "cache-us", 3600,
    vertex_project="my-project",
    vertex_location="us-central1"
)

# âœ“ ä¸¤ä¸ªç‹¬ç«‹çš„ç¼“å­˜
```

### 3. æä¾›å•†éš”ç¦»

```python
cache_key = "doc-hash-123"

# Google AI Studio (Gemini)
cache_manager.set_cache(
    cache_key, "gemini-cache", 3600,
    custom_llm_provider="gemini"
)

# Vertex AI
cache_manager.set_cache(
    cache_key, "vertex-cache", 3600,
    vertex_project="my-project",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)

# âœ“ ä¸¤ä¸ªç‹¬ç«‹çš„ç¼“å­˜
```

## æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
cd /Users/lizhen/go/src/github.com/litellm

# è¿è¡Œå¤šé¡¹ç›®æµ‹è¯•
python3 test_cache_scoping_standalone.py
```

### é¢„æœŸè¾“å‡º

```
ğŸš€ Multi-Project Cache Scoping Tests
================================================================================

Test 1: Basic Project/Location Scoping
âœ“ Test passed! 2 independent cache entries created

Scoped keys:
  - content-hash-123:project-1:global:7c0ff9df2f051a2d
  - content-hash-123:project-2:global:225155362746ff4a

Test 2: Your Actual Multi-Project Configuration
âœ“ gemini-qn-bz (global): projects/gemini-qn-bz/.../123
âœ“ gemini-prod (global): projects/gemini-prod/.../456
âœ“ gemini-dev (us-central1): projects/gemini-dev/.../789

ğŸ‰ ALL TESTS PASSED!
```

## æ€§èƒ½å½±å“

### å†…å­˜å ç”¨

```python
# æ¯ä¸ªç¼“å­˜æ¡ç›®
base_entry = ~150 bytes

# å¤šé¡¹ç›®åœºæ™¯ï¼ˆç›¸åŒå†…å®¹ï¼Œ3ä¸ªé¡¹ç›®ï¼‰
3_projects Ã— 150_bytes = 450 bytes

# 100 ä¸ªä¸åŒå†…å®¹ Ã— 3 ä¸ªé¡¹ç›®
100 Ã— 3 Ã— 150 = 45KB  # å¯å¿½ç•¥ä¸è®¡
```

### æŸ¥æ‰¾æ€§èƒ½

```python
# å“ˆå¸ŒæŸ¥æ‰¾ï¼šO(1)
# å³ä½¿æœ‰å¤šä¸ªé¡¹ç›®ï¼ŒæŸ¥æ‰¾é€Ÿåº¦ä¸å—å½±å“

# å•é¡¹ç›®ï¼šO(1)
# å¤šé¡¹ç›®ï¼šO(1)  # ç›¸åŒï¼
```

## å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: å¤šç¯å¢ƒéƒ¨ç½²

```python
# å¼€å‘ç¯å¢ƒ
dev_response = completion(
    model="gemini-2.0-flash-dev",
    messages=messages_with_cache
)
# ç¼“å­˜åœ¨: gemini-dev project

# ç”Ÿäº§ç¯å¢ƒ
prod_response = completion(
    model="gemini-2.0-flash-prod",
    messages=messages_with_cache
)
# ç¼“å­˜åœ¨: gemini-prod project

# âœ“ ä¸¤ä¸ªç¯å¢ƒå®Œå…¨ç‹¬ç«‹
# âœ“ äº’ä¸å½±å“
```

### æ¡ˆä¾‹ 2: å¤šç§Ÿæˆ·åº”ç”¨

```python
# ç§Ÿæˆ· A
tenant_a_response = completion(
    model="gemini-tenant-a",  # æ˜ å°„åˆ° project-a
    messages=messages
)

# ç§Ÿæˆ· B
tenant_b_response = completion(
    model="gemini-tenant-b",  # æ˜ å°„åˆ° project-b
    messages=messages
)

# âœ“ æ¯ä¸ªç§Ÿæˆ·çš„ç¼“å­˜éš”ç¦»
# âœ“ æ•°æ®å®‰å…¨
```

### æ¡ˆä¾‹ 3: åœ°ç†åˆ†å¸ƒ

```python
# äºšæ´²ç”¨æˆ·
asia_response = completion(
    model="gemini-asia",  # project-asia, asia-northeast1
    messages=messages
)

# æ¬§æ´²ç”¨æˆ·
eu_response = completion(
    model="gemini-eu",  # project-eu, europe-west1
    messages=messages
)

# âœ“ æ¯ä¸ªåŒºåŸŸç‹¬ç«‹ç¼“å­˜
# âœ“ é™ä½è·¨åŒºåŸŸå»¶è¿Ÿ
```

## ç›‘æ§å’Œè°ƒè¯•

### æŸ¥çœ‹æ‰€æœ‰ç¼“å­˜

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

cache_manager = get_cache_manager()
stats = cache_manager.get_stats()

print(f"æ€»ç¼“å­˜æ¡ç›®: {stats['total_entries']}")
print(f"æœ‰æ•ˆæ¡ç›®: {stats['valid_entries']}")

# æŸ¥çœ‹ä½œç”¨åŸŸé”®
for key in stats['cache_keys']:
    print(f"  {key}")
    # è¾“å‡ºç¤ºä¾‹:
    # content-hash:gemini-qn-bz:global:7c0ff9df
    # content-hash:gemini-prod:global:225155362
```

### æŒ‰é¡¹ç›®ç»Ÿè®¡

```python
def get_project_stats(manager):
    """ç»Ÿè®¡æ¯ä¸ªé¡¹ç›®çš„ç¼“å­˜æ•°é‡"""
    stats = manager.get_stats()
    project_counts = {}

    for key in stats['cache_keys']:
        if ':' in key:
            parts = key.split(':')
            if len(parts) >= 3:
                project = parts[1]
                project_counts[project] = project_counts.get(project, 0) + 1

    return project_counts

# ä½¿ç”¨
counts = get_project_stats(cache_manager)
for project, count in counts.items():
    print(f"{project}: {count} caches")

# è¾“å‡º:
# gemini-qn-bz: 15 caches
# gemini-prod: 23 caches
# gemini-dev: 8 caches
```

### æ¸…ç†ç‰¹å®šé¡¹ç›®çš„ç¼“å­˜

```python
def clear_project_caches(project_name: str):
    """æ¸…ç†ç‰¹å®šé¡¹ç›®çš„æ‰€æœ‰ç¼“å­˜"""
    cache_manager = get_cache_manager()
    stats = cache_manager.get_stats()

    cleared = 0
    for key in stats['cache_keys']:
        if f":{project_name}:" in key:
            # æå–åŸå§‹ cache_key
            base_key = key.split(':')[0]
            # å¤±æ•ˆç¼“å­˜ï¼ˆéœ€è¦çŸ¥é“ locationï¼‰
            # è¿™ä¸ªä¾‹å­å‡è®¾ global location
            cache_manager.invalidate_cache(
                cache_key=base_key,
                vertex_project=project_name,
                vertex_location="global",
                custom_llm_provider="vertex_ai"
            )
            cleared += 1

    return cleared

# ä½¿ç”¨
cleared = clear_project_caches("gemini-dev")
print(f"æ¸…ç†äº† {cleared} ä¸ª gemini-dev çš„ç¼“å­˜")
```

## å¸¸è§é—®é¢˜

### Q1: å¦‚æœå¿˜è®°ä¼ é€’ vertex_project ä¼šæ€æ ·ï¼Ÿ

**A**: ä¼šå›é€€åˆ°æ— ä½œç”¨åŸŸçš„ç¼“å­˜é”®ï¼Œå¯èƒ½å¯¼è‡´ä¸åŒé¡¹ç›®å…±äº«ç¼“å­˜ï¼ˆä¸æ¨èï¼‰ã€‚

```python
# âŒ é”™è¯¯ï¼šæ²¡æœ‰ä¼ é€’é¡¹ç›®ä¿¡æ¯
cache_manager.set_cache("key", "id", 3600)

# âœ… æ­£ç¡®ï¼šä¼ é€’å®Œæ•´ä½œç”¨åŸŸ
cache_manager.set_cache(
    "key", "id", 3600,
    vertex_project="my-project",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)
```

### Q2: èƒ½å¦è·¨é¡¹ç›®å…±äº«ç¼“å­˜ï¼Ÿ

**A**: ä¸èƒ½ï¼Œä¹Ÿä¸åº”è¯¥ã€‚Google Vertex AI çš„ç¼“å­˜æ˜¯é¡¹ç›®çº§åˆ«çš„ï¼Œæ— æ³•è·¨é¡¹ç›®å…±äº«ã€‚

### Q3: å¦‚ä½•è¿ç§»é¡¹ç›®ï¼Ÿ

**A**: ç¼“å­˜ä¼šè‡ªåŠ¨å¤±æ•ˆï¼Œæ–°é¡¹ç›®ä¼šåˆ›å»ºæ–°ç¼“å­˜ã€‚

```python
# æ—§é…ç½®
# model: vertex_ai/gemini-2.0-flash-001
# vertex_project: "old-project"

# æ–°é…ç½®
# vertex_project: "new-project"

# ç»“æœï¼š
# - old-project çš„æœ¬åœ°ç¼“å­˜ä¼šè‡ªåŠ¨è¿‡æœŸ
# - new-project ä¼šåˆ›å»ºæ–°çš„ç¼“å­˜
# - æ— éœ€æ‰‹åŠ¨è¿ç§»
```

### Q4: å¤šä¸ªåŒºåŸŸä¼šå¢åŠ æˆæœ¬å—ï¼Ÿ

**A**: æ˜¯çš„ï¼ŒåŒæ ·çš„å†…å®¹åœ¨ä¸åŒåŒºåŸŸéœ€è¦åˆ†åˆ«ç¼“å­˜ï¼Œä¼šå ç”¨å„è‡ªçš„é…é¢ã€‚å»ºè®®ï¼š

```python
# ç­–ç•¥ 1: æŒ‰åŒºåŸŸè·¯ç”±ç”¨æˆ·
if user_region == "asia":
    model = "gemini-asia"  # asia-northeast1
elif user_region == "europe":
    model = "gemini-eu"    # europe-west1

# ç­–ç•¥ 2: ä½¿ç”¨ global åŒºåŸŸ
# æ‰€æœ‰é¡¹ç›®ä½¿ç”¨ global location å¯ä»¥å‡å°‘ç¼“å­˜å‰¯æœ¬
```

## æœ€ä½³å®è·µ

### 1. æ¸…æ™°çš„å‘½åçº¦å®š

```yaml
model_list:
  # æ ¼å¼: {service}-{project}-{region}
  - model_name: gemini-bz-global
    litellm_params:
      vertex_project: "gemini-qn-bz"
      vertex_location: "global"

  - model_name: gemini-prod-global
    litellm_params:
      vertex_project: "gemini-prod"
      vertex_location: "global"
```

### 2. ç»Ÿä¸€çš„ Location ç­–ç•¥

```yaml
# âœ… æ¨èï¼šç»Ÿä¸€ä½¿ç”¨ global
# - å‡å°‘ç¼“å­˜å‰¯æœ¬
# - ç®€åŒ–ç®¡ç†
model_list:
  - model_name: gemini-bz
    litellm_params:
      vertex_project: "gemini-qn-bz"
      vertex_location: "global"  # ç»Ÿä¸€

  - model_name: gemini-prod
    litellm_params:
      vertex_project: "gemini-prod"
      vertex_location: "global"  # ç»Ÿä¸€
```

### 3. ç›‘æ§ç¼“å­˜ä½¿ç”¨

```python
# å®šæœŸç›‘æ§
import schedule

def monitor_cache():
    stats = get_cache_manager().get_stats()
    print(f"[{time.now()}] ç¼“å­˜ç»Ÿè®¡: {stats['valid_entries']} ä¸ªæœ‰æ•ˆæ¡ç›®")

    # æŒ‰é¡¹ç›®ç»Ÿè®¡
    counts = get_project_stats(get_cache_manager())
    for proj, count in counts.items():
        print(f"  {proj}: {count}")

# æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
schedule.every(1).hours.do(monitor_cache)
```

### 4. æµ‹è¯•æ–°é¡¹ç›®

```python
# æ·»åŠ æ–°é¡¹ç›®å‰å…ˆæµ‹è¯•
def test_new_project(project_name, location):
    """æµ‹è¯•æ–°é¡¹ç›®çš„ç¼“å­˜æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    cache_manager = get_cache_manager()

    test_key = f"test-{time.time()}"
    test_id = f"projects/{project_name}/locations/{location}/cachedContents/test"

    # è®¾ç½®
    cache_manager.set_cache(
        test_key, test_id, 60,
        vertex_project=project_name,
        vertex_location=location,
        custom_llm_provider="vertex_ai"
    )

    # è·å–
    retrieved = cache_manager.get_cache(
        test_key,
        vertex_project=project_name,
        vertex_location=location,
        custom_llm_provider="vertex_ai"
    )

    assert retrieved == test_id, "ç¼“å­˜æµ‹è¯•å¤±è´¥"
    print(f"âœ“ {project_name} ({location}) ç¼“å­˜æµ‹è¯•é€šè¿‡")

# ä½¿ç”¨
test_new_project("gemini-new-proj", "global")
```

## æ€»ç»“

âœ… **è‡ªåŠ¨éš”ç¦»**ï¼šé¡¹ç›® + åŒºåŸŸè‡ªåŠ¨ä½œä¸ºä½œç”¨åŸŸ
âœ… **é›¶é…ç½®**ï¼šæ— éœ€é¢å¤–è®¾ç½®
âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šå“ˆå¸ŒæŸ¥æ‰¾ï¼ŒO(1) å¤æ‚åº¦
âœ… **å†…å­˜å‹å¥½**ï¼šå¤šé¡¹ç›®åœºæ™¯å¢åŠ çš„å†…å­˜å¯å¿½ç•¥
âœ… **æ˜“äºè°ƒè¯•**ï¼šæ¸…æ™°çš„ä½œç”¨åŸŸé”®æ ¼å¼
âœ… **ç”Ÿäº§å°±ç»ª**ï¼šç»è¿‡å®Œæ•´æµ‹è¯•éªŒè¯

æ‚¨çš„å¤šé¡¹ç›®é…ç½®å·²å®Œå…¨æ”¯æŒï¼ğŸ‰
