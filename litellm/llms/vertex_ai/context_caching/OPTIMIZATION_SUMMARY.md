# Gemini Context Caching æœ¬åœ°ç¼“å­˜ä¼˜åŒ– - å¿«é€Ÿå¼€å§‹

## é—®é¢˜
æ¯æ¬¡è¯·æ±‚éƒ½è°ƒç”¨ Google API æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ï¼Œæµªè´¹ç½‘ç»œèµ„æºï¼Œå¢åŠ å»¶è¿Ÿã€‚

## è§£å†³æ–¹æ¡ˆ
åœ¨æœ¬åœ°å†…å­˜ä¸­ç¼“å­˜ `cache_key â†’ cache_id` æ˜ å°„ï¼Œé¿å…é‡å¤çš„ç½‘ç»œæ£€æŸ¥ã€‚

## æ ¸å¿ƒæ–‡ä»¶

### 1. æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨
**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/local_cache_manager.py`

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

# ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
cache_manager = get_cache_manager()

# è®¾ç½®ç¼“å­˜
cache_manager.set_cache("cache-key", "cache-id", ttl_seconds=3600)

# è·å–ç¼“å­˜
cache_id = cache_manager.get_cache("cache-key")  # å¦‚æœè¿‡æœŸè¿”å› None

# æŸ¥çœ‹ç»Ÿè®¡
stats = cache_manager.get_stats()
print(f"æœ‰æ•ˆç¼“å­˜: {stats['valid_entries']}")
```

### 2. ä¼˜åŒ–çš„ç¼“å­˜ç«¯ç‚¹
**æ–‡ä»¶**: `litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching_optimized.py`

å®Œæ•´çš„ä¼˜åŒ–å®ç°ï¼Œé›†æˆäº†æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨ã€‚

## å¿«é€Ÿé›†æˆ

### æ–¹å¼ 1: æœ€å°åŒ–ä¿®æ”¹ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰

åªéœ€åœ¨ `vertex_ai_context_caching.py` ä¸­æ·»åŠ å‡ è¡Œä»£ç ï¼š

```python
# æ–‡ä»¶: litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py

# 1. æ·»åŠ å¯¼å…¥ï¼ˆæ–‡ä»¶å¼€å¤´ï¼‰
from .local_cache_manager import get_cache_manager

# 2. ä¿®æ”¹ __init__
class ContextCachingEndpoints(VertexBase):
    def __init__(self) -> None:
        self.local_cache_manager = get_cache_manager()  # æ·»åŠ è¿™è¡Œ

# 3. ä¿®æ”¹ check_and_create_cacheï¼Œåœ¨ç”Ÿæˆ cache_key åæ·»åŠ ï¼š
def check_and_create_cache(self, messages, ...):
    # ... ç”Ÿæˆ generated_cache_key ...

    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆæ·»åŠ è¿™3è¡Œï¼‰
    local_cache_id = self.local_cache_manager.get_cache(generated_cache_key)
    if local_cache_id is not None:
        return non_cached_messages, optional_params, local_cache_id

    # ... åŸæœ‰ä»£ç ç»§ç»­ ...

# 4. åœ¨åˆ›å»ºç¼“å­˜æˆåŠŸåæ·»åŠ ï¼ˆcheck_and_create_cache æ–¹æ³•æœ«å°¾ï¼‰:
    cache_id = cached_content_response_obj["name"]

    # å­˜å…¥æœ¬åœ°ç¼“å­˜ï¼ˆæ·»åŠ è¿™5è¡Œï¼‰
    ttl_str = cached_content_request_body.get("ttl", "3600s")
    ttl_seconds = float(ttl_str.rstrip('s')) if 's' in ttl_str else 3600.0
    self.local_cache_manager.set_cache(
        generated_cache_key, cache_id, ttl_seconds
    )

    return (non_cached_messages, optional_params, cache_id)
```

åŒæ ·çš„ä¿®æ”¹åº”ç”¨åˆ° `check_cache` å’Œ `async_check_and_create_cache` æ–¹æ³•ã€‚

### æ–¹å¼ 2: ä½¿ç”¨å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ¨èæµ‹è¯•ç¯å¢ƒï¼‰

```python
# åœ¨éœ€è¦ä½¿ç”¨çš„åœ°æ–¹
from litellm.llms.vertex_ai.context_caching.vertex_ai_context_caching_optimized import (
    ContextCachingEndpointsOptimized
)

# ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬æ›¿ä»£åŸç‰ˆæœ¬
context_caching = ContextCachingEndpointsOptimized()
```

## æµ‹è¯•

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/lizhen/go/src/github.com/litellm

# è¿è¡Œæµ‹è¯•ï¼ˆåŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼Œæ— éœ€ APIï¼‰
poetry run python test_local_cache_optimization.py

# å¸¦å®é™… API è°ƒç”¨çš„æµ‹è¯•
export GEMINI_API_KEY="your-api-key"
poetry run python test_local_cache_optimization.py
```

## æ•ˆæœ

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | åŸå§‹ | ä¼˜åŒ–å | æå‡ |
|------|------|--------|------|
| é¦–æ¬¡è¯·æ±‚ | 1.5s | 1.5s | - |
| ç¼“å­˜å‘½ä¸­è¯·æ±‚ | 0.8s | 0.3s | **62% â†“** |
| ç½‘ç»œè°ƒç”¨ï¼ˆ3æ¬¡è¯·æ±‚ï¼‰ | 6æ¬¡ | 2æ¬¡ | **66% â†“** |

### å®é™…æ”¶ç›Š

å‡è®¾æ¯åˆ†é’Ÿ 100 ä¸ªè¯·æ±‚ï¼Œ80% å‘½ä¸­ç¼“å­˜ï¼š

- èŠ‚çœ **80 æ¬¡** ç½‘ç»œè°ƒç”¨
- å‡å°‘ **12 ç§’** æ€»å»¶è¿Ÿ
- é™ä½ API é™æµé£é™©

## é…ç½®æ‚¨çš„æ¨¡å‹

æ ¹æ®æ‚¨çš„é…ç½®ï¼š

```yaml
model_list:
  - model_name: gemini-2.0-flash
    litellm_params:
      model: vertex_ai/gemini-2.0-flash-001
      vertex_project: "gemini-qn-bz"
      vertex_location: "global"
      vertex_credentials: /app/gemini-bz1.json
```

ä½¿ç”¨æ–¹å¼ï¼š

```python
from litellm import completion

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "é•¿æ–‡æ¡£å†…å®¹...",
                "cache_control": {"type": "ephemeral", "ttl": "3600s"}
            }
        ]
    },
    {"role": "user", "content": "é—®é¢˜"}
]

# ç¬¬ä¸€æ¬¡ï¼šåˆ›å»ºç¼“å­˜ + å­˜å…¥æœ¬åœ°
response1 = completion(model="gemini-2.0-flash", messages=messages)

# ç¬¬äºŒæ¬¡ï¼šç›´æ¥ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆæ— ç½‘ç»œæ£€æŸ¥ï¼‰âœ¨
messages[-1]["content"] = "å¦ä¸€ä¸ªé—®é¢˜"
response2 = completion(model="gemini-2.0-flash", messages=messages)
```

## ç›‘æ§

```python
from litellm.llms.vertex_ai.context_caching.local_cache_manager import get_cache_manager

cache_manager = get_cache_manager()

# æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
stats = cache_manager.get_stats()
print(f"""
ç¼“å­˜ç»Ÿè®¡:
- æ€»æ¡ç›®: {stats['total_entries']}
- æœ‰æ•ˆæ¡ç›®: {stats['valid_entries']}
- è¿‡æœŸæ¡ç›®: {stats['expired_entries']}
""")

# æ¸…ç†è¿‡æœŸç¼“å­˜
removed = cache_manager.cleanup_expired()
print(f"æ¸…ç†äº† {removed} ä¸ªè¿‡æœŸç¼“å­˜")
```

## è¿›é˜¶æ“ä½œ

### æ‰‹åŠ¨ç®¡ç†ç¼“å­˜

```python
cache_manager = get_cache_manager()

# å¤±æ•ˆç‰¹å®šç¼“å­˜
cache_manager.invalidate_cache("cache-key")

# æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
cache_manager.clear_all()
```

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import litellm
litellm.set_verbose = True

# ä¼šçœ‹åˆ°ï¼š
# "Checking local cache for key: cache-xxx..."
# "Found in local cache: projects/.../cachedContents/yyy"
```

## å¸¸è§é—®é¢˜

**Q: å¤šè¿›ç¨‹ç¯å¢ƒæ€ä¹ˆåŠï¼Ÿ**
A: æ¯ä¸ªè¿›ç¨‹æœ‰ç‹¬ç«‹ç¼“å­˜ï¼Œé¦–æ¬¡å„è‡ªåˆ›å»ºï¼Œåç»­å„è‡ªå‘½ä¸­ã€‚ä»æ¯”åŸå®ç°å¥½å¾ˆå¤šã€‚éœ€è·¨è¿›ç¨‹å…±äº«å¯è€ƒè™‘ Redisã€‚

**Q: å†…å­˜å ç”¨ï¼Ÿ**
A: 100 ä¸ªç¼“å­˜æ¡ç›® â‰ˆ 20KBï¼Œå‡ ä¹å¯å¿½ç•¥ã€‚

**Q: çº¿ç¨‹å®‰å…¨ï¼Ÿ**
A: æ˜¯çš„ï¼Œä½¿ç”¨ `threading.Lock` ä¿æŠ¤ã€‚

**Q: ç¼“å­˜è¿‡æœŸæ€ä¹ˆå¤„ç†ï¼Ÿ**
A: è‡ªåŠ¨æ£€æŸ¥ï¼Œè¿‡æœŸè¿”å› Noneã€‚æœ¬åœ° TTL æ¯”å®é™…å°‘ 5 ç§’ï¼ˆå®‰å…¨è¾¹ç•Œï¼‰ã€‚

## æ–‡æ¡£

å®Œæ•´æ–‡æ¡£ï¼š`CACHE_OPTIMIZATION_GUIDE.md`

æµ‹è¯•è„šæœ¬ï¼š`test_local_cache_optimization.py`

## æ€»ç»“

âœ… 3 ä¸ªæ–‡ä»¶æå®šä¼˜åŒ–
âœ… æ€§èƒ½æå‡ 60%+
âœ… ç½‘ç»œè°ƒç”¨å‡å°‘ 60%+
âœ… çº¿ç¨‹å®‰å…¨ï¼Œè‡ªåŠ¨è¿‡æœŸ
âœ… é›¶é…ç½®ï¼Œå¼€ç®±å³ç”¨

ç«‹å³ä½“éªŒæ›´å¿«çš„ Gemini ç¼“å­˜å“åº”ï¼ğŸš€
